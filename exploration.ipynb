{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from dash import dcc, html\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional, Union\n",
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "import json\n",
    "from utils import OpenMLTaskHandler\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import io\n",
    "import base64\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from flask import Flask, render_template, jsonify, request, send_file\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_plot(dataset_results):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(\n",
    "        data=dataset_results,\n",
    "        x=\"framework\",\n",
    "        y=\"result\",\n",
    "        hue=\"metric\",\n",
    "        palette=\"muted\",\n",
    "    )\n",
    "    plt.title(\"Results by Framework and Metric\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot to buffer\n",
    "    buffer = io.BytesIO()\n",
    "    plt.savefig(buffer, format=\"png\")\n",
    "    buffer.seek(0)\n",
    "    encoded_image = base64.b64encode(buffer.read()).decode()\n",
    "    buffer.close()\n",
    "    plt.close()\n",
    "\n",
    "    return f\"data:image/png;base64,{encoded_image}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_load_file(file_path, file_type) -> Union[pd.DataFrame, dict, None]:\n",
    "        \"\"\"\n",
    "        This function is responsible for safely loading a file. It returns None if the file is not found or if there is an error loading the file.\n",
    "        \"\"\"\n",
    "        if file_type == \"json\":\n",
    "            try:\n",
    "                with open(str(Path(file_path)), \"r\") as f:\n",
    "                    return json.load(f)\n",
    "            except:\n",
    "                return None\n",
    "        elif file_type == \"pd\":\n",
    "            try:\n",
    "                return pd.read_csv(str(file_path))\n",
    "            except:\n",
    "                return None\n",
    "        elif file_type == \"textdict\":\n",
    "            try:\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    return json.loads(f.read())\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import openml\n",
    "\n",
    "class ResultCollector:\n",
    "    def __init__(self):\n",
    "        self.experiment_directory = Path(\"./data/results/*\")\n",
    "\n",
    "        self.all_run_paths = glob(pathname=str(self.experiment_directory))\n",
    "        self.all_results = pd.DataFrame()\n",
    "        self.openml_task_handler = OpenMLTaskHandler()\n",
    "        # Required columns\n",
    "        self.required_columns = {\n",
    "            \"metric\",\n",
    "            \"result\",\n",
    "            \"framework\",\n",
    "            \"dataset_id\",\n",
    "            \"id\",\n",
    "            \"task\",\n",
    "            \"predict_duration\",\n",
    "            \"models\",\n",
    "        }\n",
    "\n",
    "        # Define how to find the best result for the metric\n",
    "        self.metric_used_dict = {\n",
    "            \"auc\": lambda x: x.max(),\n",
    "            \"neg_logloss\": lambda x: x.min(),\n",
    "        }\n",
    "    \n",
    "    def get_dataset_description_from_id(self, dataset_id: int) -> Optional[str]:\n",
    "        return openml.datasets.get_dataset(dataset_id).description\n",
    "\n",
    "    def collect_all_run_info_to_df(self):\n",
    "        \"\"\"\n",
    "        This function is responsible for loading all the results files from the runs and storing them in self.all_results. This is further used to generate the dashboard.\n",
    "        \"\"\"\n",
    "        all_results_list = []  # Temporary list to store individual DataFrames\n",
    "\n",
    "        for run_path in tqdm(self.all_run_paths, total=len(self.all_run_paths)):\n",
    "            run_path = Path(run_path)\n",
    "            results_file_path = run_path / \"results.csv\"\n",
    "\n",
    "            # Load results file if it exists\n",
    "            results_file = safe_load_file(results_file_path, \"pd\")\n",
    "\n",
    "            # If results file is loaded, proceed to process it\n",
    "            if results_file is not None:\n",
    "                # Get the model path specific to this run_path\n",
    "                models_path_list = list((run_path / \"models\").rglob(\"models.*\"))\n",
    "                leaderboard_path_list = list(\n",
    "                    (run_path / \"models\").rglob(\"leaderboard.*\")\n",
    "                )\n",
    "                # models_path = str(models_path_list[0]) if len(models_path_list) >0 else None\n",
    "\n",
    "                if len(models_path_list) > 0:\n",
    "                    models_path = str(models_path_list[0])\n",
    "                elif len(leaderboard_path_list) > 0:\n",
    "                    models_path = str(leaderboard_path_list[0])\n",
    "                else:\n",
    "                    models_path = None\n",
    "\n",
    "                # Add the model path as a new column in the current results_file DataFrame\n",
    "                results_file[\"models\"] = models_path\n",
    "\n",
    "                # Get the dataset ID for each row in the results file\n",
    "                results_file[\"dataset_id\"] = results_file[\"id\"].apply(\n",
    "                    self.openml_task_handler.get_dataset_id_from_task_id\n",
    "                )\n",
    "                results_file[\"dataset_description\"] = results_file[\"dataset_id\"].apply(\n",
    "                    self.get_dataset_description_from_id\n",
    "                )\n",
    "\n",
    "                # Append the processed DataFrame to our list\n",
    "                all_results_list.append(results_file)\n",
    "\n",
    "        # Concatenate all individual DataFrames into self.all_results\n",
    "        if all_results_list:\n",
    "            self.all_results = pd.concat(all_results_list, ignore_index=True)\n",
    "    \n",
    "    def validate_dataframe_and_add_extra_info(self):\n",
    "        # Validate DataFrame\n",
    "        if self.all_results is None or self.all_results.empty:\n",
    "            return \"Error: Provided DataFrame is empty or None.\"\n",
    "\n",
    "        # Handle duplicate frameworks by keeping the one with the best result\n",
    "        self.all_results = self.all_results.drop_duplicates(subset=[\"framework\"], keep=\"first\")\n",
    "\n",
    "        # Add missing columns with default values\n",
    "        for column in self.required_columns:\n",
    "            if column not in self.all_results.columns:\n",
    "                self.all_results[column] = \"N/A\"\n",
    "        \n",
    "    def __call__(self):\n",
    "        self.collect_all_run_info_to_df()\n",
    "        # self.validate_dataframe_and_add_extra_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/65 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:00<00:00, 310.14it/s]\n"
     ]
    }
   ],
   "source": [
    "collector = ResultCollector()\n",
    "collector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>task</th>\n",
       "      <th>framework</th>\n",
       "      <th>constraint</th>\n",
       "      <th>fold</th>\n",
       "      <th>type</th>\n",
       "      <th>result</th>\n",
       "      <th>metric</th>\n",
       "      <th>mode</th>\n",
       "      <th>version</th>\n",
       "      <th>...</th>\n",
       "      <th>models_count</th>\n",
       "      <th>seed</th>\n",
       "      <th>info</th>\n",
       "      <th>acc</th>\n",
       "      <th>balacc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>models</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>dataset_description</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openml.org/t/9</td>\n",
       "      <td>autos</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neg_logloss</td>\n",
       "      <td>docker</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1769202869</td>\n",
       "      <td>ValueError: 7 columns passed, passed data had ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>**Author**: Jeffrey C. Schlimmer (Jeffrey.Schl...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openml.org/t/9</td>\n",
       "      <td>autos</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neg_logloss</td>\n",
       "      <td>docker</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1769202870</td>\n",
       "      <td>ValueError: 7 columns passed, passed data had ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>**Author**: Jeffrey C. Schlimmer (Jeffrey.Schl...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openml.org/t/5</td>\n",
       "      <td>arrhythmia</td>\n",
       "      <td>GAMA</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neg_logloss</td>\n",
       "      <td>docker</td>\n",
       "      <td>23.0.0.post1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1490456114</td>\n",
       "      <td>NoResultError: population must be at least siz...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>**Author**: H. Altay Guvenir, Burak Acar, Hald...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>openml.org/t/2</td>\n",
       "      <td>anneal</td>\n",
       "      <td>H2OAutoML</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neg_logloss</td>\n",
       "      <td>docker</td>\n",
       "      <td>3.46.0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>246905805</td>\n",
       "      <td>ResultError: y contains previously unseen labe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data/results/h2oautoml.openml_t_2.test.docker....</td>\n",
       "      <td>2</td>\n",
       "      <td>**Author**: Unknown. Donated by David Sterling...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openml.org/t/6</td>\n",
       "      <td>letter</td>\n",
       "      <td>hyperoptsklearn</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neg_logloss</td>\n",
       "      <td>docker</td>\n",
       "      <td>latest</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1111474048</td>\n",
       "      <td>CalledProcessError: Command '/bench/frameworks...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>**Author**: David J. Slate  \\n**Source**: [UCI...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>openml.org/t/6</td>\n",
       "      <td>letter</td>\n",
       "      <td>autosklearn</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>-0.106761</td>\n",
       "      <td>neg_logloss</td>\n",
       "      <td>docker</td>\n",
       "      <td>0.15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2055450922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.967701</td>\n",
       "      <td>0.106761</td>\n",
       "      <td>data/results/autosklearn.openml_t_6.test.docke...</td>\n",
       "      <td>6</td>\n",
       "      <td>**Author**: David J. Slate  \\n**Source**: [UCI...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>openml.org/t/2</td>\n",
       "      <td>anneal</td>\n",
       "      <td>GAMA</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neg_logloss</td>\n",
       "      <td>docker</td>\n",
       "      <td>23.0.0.post1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1559501364</td>\n",
       "      <td>ValueError: 6 columns passed, passed data had ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>**Author**: Unknown. Donated by David Sterling...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>openml.org/t/5</td>\n",
       "      <td>arrhythmia</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neg_logloss</td>\n",
       "      <td>docker</td>\n",
       "      <td>1.2.2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>995862599</td>\n",
       "      <td>ValueError: 16 columns passed, passed data had...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>**Author**: H. Altay Guvenir, Burak Acar, Hald...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>openml.org/t/5</td>\n",
       "      <td>arrhythmia</td>\n",
       "      <td>H2OAutoML</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neg_logloss</td>\n",
       "      <td>docker</td>\n",
       "      <td>3.46.0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250190127</td>\n",
       "      <td>ValueError: 16 columns passed, passed data had...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data/results/h2oautoml.openml_t_5.test.docker....</td>\n",
       "      <td>5</td>\n",
       "      <td>**Author**: H. Altay Guvenir, Burak Acar, Hald...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>openml.org/t/11</td>\n",
       "      <td>balance-scale</td>\n",
       "      <td>autosklearn</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>-0.017457</td>\n",
       "      <td>neg_logloss</td>\n",
       "      <td>docker</td>\n",
       "      <td>0.15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2048131128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017457</td>\n",
       "      <td>data/results/autosklearn.openml_t_11.test.dock...</td>\n",
       "      <td>11</td>\n",
       "      <td>**Author**: Siegler, R. S. (donated by Tim Hum...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id           task        framework constraint  fold  \\\n",
       "0    openml.org/t/9          autos     RandomForest       test     0   \n",
       "1    openml.org/t/9          autos     RandomForest       test     1   \n",
       "2    openml.org/t/5     arrhythmia             GAMA       test     0   \n",
       "3    openml.org/t/2         anneal        H2OAutoML       test     0   \n",
       "4    openml.org/t/6         letter  hyperoptsklearn       test     0   \n",
       "..              ...            ...              ...        ...   ...   \n",
       "64   openml.org/t/6         letter      autosklearn       test     0   \n",
       "65   openml.org/t/2         anneal             GAMA       test     0   \n",
       "66   openml.org/t/5     arrhythmia     DecisionTree       test     0   \n",
       "67   openml.org/t/5     arrhythmia        H2OAutoML       test     0   \n",
       "68  openml.org/t/11  balance-scale      autosklearn       test     0   \n",
       "\n",
       "          type    result       metric    mode       version  ... models_count  \\\n",
       "0   multiclass       NaN  neg_logloss  docker           1.0  ...          NaN   \n",
       "1   multiclass       NaN  neg_logloss  docker           1.0  ...          NaN   \n",
       "2   multiclass       NaN  neg_logloss  docker  23.0.0.post1  ...          NaN   \n",
       "3   multiclass       NaN  neg_logloss  docker      3.46.0.6  ...         46.0   \n",
       "4   multiclass       NaN  neg_logloss  docker        latest  ...          NaN   \n",
       "..         ...       ...          ...     ...           ...  ...          ...   \n",
       "64  multiclass -0.106761  neg_logloss  docker        0.15.0  ...          4.0   \n",
       "65  multiclass       NaN  neg_logloss  docker  23.0.0.post1  ...          NaN   \n",
       "66  multiclass       NaN  neg_logloss  docker         1.2.2  ...          NaN   \n",
       "67  multiclass       NaN  neg_logloss  docker      3.46.0.6  ...          NaN   \n",
       "68  multiclass -0.017457  neg_logloss  docker        0.15.0  ...          5.0   \n",
       "\n",
       "          seed                                               info    acc  \\\n",
       "0   1769202869  ValueError: 7 columns passed, passed data had ...    NaN   \n",
       "1   1769202870  ValueError: 7 columns passed, passed data had ...    NaN   \n",
       "2   1490456114  NoResultError: population must be at least siz...    NaN   \n",
       "3    246905805  ResultError: y contains previously unseen labe...    NaN   \n",
       "4   1111474048  CalledProcessError: Command '/bench/frameworks...    NaN   \n",
       "..         ...                                                ...    ...   \n",
       "64  2055450922                                                NaN  0.968   \n",
       "65  1559501364  ValueError: 6 columns passed, passed data had ...    NaN   \n",
       "66   995862599  ValueError: 16 columns passed, passed data had...    NaN   \n",
       "67   250190127  ValueError: 16 columns passed, passed data had...    NaN   \n",
       "68  2048131128                                                NaN  1.000   \n",
       "\n",
       "      balacc   logloss                                             models  \\\n",
       "0        NaN       NaN                                               None   \n",
       "1        NaN       NaN                                               None   \n",
       "2        NaN       NaN                                               None   \n",
       "3        NaN       NaN  data/results/h2oautoml.openml_t_2.test.docker....   \n",
       "4        NaN       NaN                                               None   \n",
       "..       ...       ...                                                ...   \n",
       "64  0.967701  0.106761  data/results/autosklearn.openml_t_6.test.docke...   \n",
       "65       NaN       NaN                                               None   \n",
       "66       NaN       NaN                                               None   \n",
       "67       NaN       NaN  data/results/h2oautoml.openml_t_5.test.docker....   \n",
       "68  1.000000  0.017457  data/results/autosklearn.openml_t_11.test.dock...   \n",
       "\n",
       "    dataset_id                                dataset_description  auc  \n",
       "0            9  **Author**: Jeffrey C. Schlimmer (Jeffrey.Schl...  NaN  \n",
       "1            9  **Author**: Jeffrey C. Schlimmer (Jeffrey.Schl...  NaN  \n",
       "2            5  **Author**: H. Altay Guvenir, Burak Acar, Hald...  NaN  \n",
       "3            2  **Author**: Unknown. Donated by David Sterling...  NaN  \n",
       "4            6  **Author**: David J. Slate  \\n**Source**: [UCI...  NaN  \n",
       "..         ...                                                ...  ...  \n",
       "64           6  **Author**: David J. Slate  \\n**Source**: [UCI...  NaN  \n",
       "65           2  **Author**: Unknown. Donated by David Sterling...  NaN  \n",
       "66           5  **Author**: H. Altay Guvenir, Burak Acar, Hald...  NaN  \n",
       "67           5  **Author**: H. Altay Guvenir, Burak Acar, Hald...  NaN  \n",
       "68          11  **Author**: Siegler, R. S. (donated by Tim Hum...  NaN  \n",
       "\n",
       "[69 rows x 26 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector.all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetResultForDataset:\n",
    "    def __init__(self, dataset_id: int):\n",
    "        self.dataset_id = dataset_id\n",
    "        self.current_results = self.get_results_for_dataset_id(self.dataset_id)\n",
    "        self.jinja_environment = Environment(\n",
    "            loader=FileSystemLoader(\"./website_assets/templates/\")\n",
    "        )\n",
    "        self.template_to_use = {\n",
    "            \"best_result\": \"best_result_table.html\",\n",
    "            \"framework_table\": \"framework_table.html\",\n",
    "            \"metric_vs_result\": \"metric_vs_result.html\",\n",
    "        }\n",
    "        binary_metrics = [\n",
    "            \"auc\",\n",
    "            \"logloss\",\n",
    "            \"acc\",\n",
    "            \"balacc\",\n",
    "        ]  # available metrics: auc (AUC), acc (Accuracy), balacc (Balanced Accuracy), pr_auc (Precision Recall AUC), logloss (Log Loss), f1, f2, f05 (F-beta scores with beta=1, 2, or 0.5), max_pce, mean_pce (Max/Mean Per-Class Error).\n",
    "        multiclass_metrics = [\n",
    "            \"logloss\",\n",
    "            \"acc\",\n",
    "            \"balacc\",\n",
    "        ]  # available metrics: same as for binary, except auc, replaced by auc_ovo (AUC One-vs-One), auc_ovr (AUC One-vs-Rest). AUC metrics and F-beta metrics are computed with weighted average.\n",
    "        regression_metrics = [\n",
    "            \"rmse\",\n",
    "            \"r2\",\n",
    "            \"mae\",\n",
    "        ]  # available metrics: mae (Mean Absolute Error), mse (Mean Squared Error), msle (Mean Squared Logarithmic Error), rmse (Root Mean Square Error), rmsle (Root Mean Square Logarithmic Error), r2 (R^2).\n",
    "        timeseries_metrics = [\n",
    "            \"mase\",\n",
    "            \"mape\",\n",
    "            \"smape\",\n",
    "            \"wape\",\n",
    "            \"rmse\",\n",
    "            \"mse\",\n",
    "            \"mql\",\n",
    "            \"wql\",\n",
    "            \"sql\",\n",
    "        ]  # available metrics: mase (Mean Absolute Scaled Error), mape (Mean Absolute Percentage Error),\n",
    "        self.all_metrics = (\n",
    "            binary_metrics\n",
    "            + multiclass_metrics\n",
    "            + regression_metrics\n",
    "            + timeseries_metrics\n",
    "        )\n",
    "\n",
    "        # run the function to get the best result\n",
    "        self.get_best_result()\n",
    "        self.framework_names = [\"Auto-sklearn\", \"H20AutoML\", \"AutoGluon\", \"All results\"]\n",
    "        self.process_fns = [\n",
    "            self.process_auto_sklearn_data(self.current_results),\n",
    "            self.get_rows_for_framework_from_df(\n",
    "                df=self.current_results, framework_name=\"H20AutoML\", top_n=10\n",
    "            ),\n",
    "            self.get_rows_for_framework_from_df(\n",
    "                df=self.current_results, framework_name=\"AutoGluon\", top_n=10\n",
    "            ),\n",
    "            self.get_rows_for_framework_from_df(\n",
    "                df=self.current_results, framework_name=\"All results\"\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def get_results_for_dataset_id(self, dataset_id: int) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        This function returns the results for a given dataset_id. If no results are found, it returns None.\n",
    "        \"\"\"\n",
    "        results_for_dataset = collector.all_results[\n",
    "            collector.all_results[\"dataset_id\"] == dataset_id\n",
    "        ]\n",
    "        if results_for_dataset.empty:\n",
    "            return None\n",
    "        return results_for_dataset\n",
    "\n",
    "    def get_best_result(self):\n",
    "        \"\"\"\n",
    "        This function returns the best result from the current_results DataFrame. It first sorts the DataFrame based on the metric used and then returns the best result.\n",
    "        \"\"\"\n",
    "        if self.current_results is None:\n",
    "            return None\n",
    "        metric_used = self.current_results[\"metric\"].iloc[0]\n",
    "        if metric_used in [\"auc\", \"acc\", \"balacc\"]:\n",
    "            # Since higher value is better we sort in descending order\n",
    "            sort_in_ascending_order = False\n",
    "        elif metric_used in [\"logloss\", \"neg_logloss\"]:\n",
    "            # Since lower value is better we sort in ascending order\n",
    "            sort_in_ascending_order = True\n",
    "        else:\n",
    "            sort_in_ascending_order = False\n",
    "\n",
    "        sorted_results = self.current_results.sort_values(\n",
    "            by=\"result\", ascending=sort_in_ascending_order\n",
    "        ).head()\n",
    "\n",
    "        best_result = sorted_results.iloc[0]\n",
    "        self.best_framework = best_result[\"framework\"]\n",
    "        self.best_metric = best_result[\"metric\"]\n",
    "        self.type_of_task = best_result[\"type\"]\n",
    "        self.dataset_id = best_result[\"dataset_id\"]\n",
    "        self.task_id = \"https://\" + best_result[\"id\"]\n",
    "        self.task_name = best_result[\"task\"]\n",
    "        self.best_result_for_metric = best_result[\"result\"]\n",
    "        self.description = best_result[\"dataset_description\"]\n",
    "\n",
    "        # all metric columns that are in the dataframe and in the list of all metrics\n",
    "        metric_columns = [\n",
    "            col for col in self.current_results.columns if col in self.all_metrics\n",
    "        ]\n",
    "        all_metrics_present = []\n",
    "        for metric in metric_columns:\n",
    "            try:\n",
    "                all_metrics_present.append(self.current_results[metric].values[0])\n",
    "            except:\n",
    "                pass\n",
    "        # \"metric , result\" for each metric\n",
    "        # self.metric_and_result = \" \".join(\n",
    "        #     [\n",
    "        #         f\"The {metric} is {self.current_results[self.current_results['metric'] == metric]['result'].values[0]} \"\n",
    "        #         for metric in metric_columns\n",
    "        #     ]\n",
    "        # )\n",
    "        self.metric_and_result = \" \".join(\n",
    "            [\n",
    "                f\"The {metric} is {result} \"\n",
    "                for metric, result in zip(metric_columns, all_metrics_present)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def generate_best_result_table(self):\n",
    "        \"\"\"\n",
    "        This function generates the best result table using the best result information.\n",
    "        \"\"\"\n",
    "        template = self.jinja_environment.get_template(\n",
    "            self.template_to_use[\"best_result\"]\n",
    "        )\n",
    "        return template.render(\n",
    "            best_framework=self.best_framework,\n",
    "            best_metric=self.best_metric,\n",
    "            type_of_task=self.type_of_task,\n",
    "            dataset_id=self.dataset_id,\n",
    "            task_id=self.task_id,\n",
    "            task_name=self.task_name,\n",
    "        )\n",
    "\n",
    "    def process_auto_sklearn_data(self, df, top_n=10):\n",
    "        auto_sklearn_rows = df[df[\"framework\"] == \"autosklearn\"]\n",
    "        auto_sklearn_data = pd.DataFrame()\n",
    "        # for each row, read the json file from the models column and get the model id and cost\n",
    "        for _, row in auto_sklearn_rows.iterrows():\n",
    "            models_path = row[\"models\"]\n",
    "            with open(models_path, \"r\") as f:\n",
    "                models_file = json.load(f)\n",
    "                for model in models_file:\n",
    "                    model_type = (\n",
    "                        \"sklearn_classifier\"\n",
    "                        if \"sklearn_classifier\" in models_file[model]\n",
    "                        else \"sklearn_regressor\"\n",
    "                    )\n",
    "\n",
    "                    auto_sklearn_data = pd.concat(\n",
    "                        [auto_sklearn_data, pd.DataFrame([models_file[model]])],\n",
    "                        ignore_index=True,\n",
    "                    )\n",
    "\n",
    "        auto_sklearn_data = auto_sklearn_data.sort_values(\n",
    "            by=\"cost\", ascending=True\n",
    "        ).head(top_n)\n",
    "\n",
    "        return auto_sklearn_data.to_html()\n",
    "\n",
    "    def get_rows_for_framework_from_df(\n",
    "        self, df: pd.DataFrame, framework_name, top_n=40\n",
    "    ):\n",
    "        try:\n",
    "            if framework_name == \"All results\":\n",
    "                # drop the description column if it exists\n",
    "                try:\n",
    "                    df.drop(\"dataset_description\", axis=1, inplace=True)\n",
    "                except:\n",
    "                    pass\n",
    "                return df.to_html()\n",
    "            framework_rows: pd.DataFrame = df[df[\"framework\"] == framework_name][\n",
    "                \"models\"\n",
    "            ].values[0]\n",
    "            framework_data = safe_load_file(framework_rows, \"pd\")\n",
    "            if top_n is not None:\n",
    "                framework_data = framework_data.head(40)\n",
    "\n",
    "            return framework_data.to_html()\n",
    "        except:\n",
    "            return \"No data found for this framework.\"\n",
    "\n",
    "    def generate_framework_table(self):\n",
    "        \"\"\"\n",
    "        This function generates the framework table using the framework_name information.\n",
    "        \"\"\"\n",
    "        # template = self.jinja_environment.get_template(\n",
    "        #     self.template_to_use[\"framework_table\"]\n",
    "        # )\n",
    "        complete_html = \"\"\n",
    "        for framework_name, process_fn in zip(self.framework_names, self.process_fns):\n",
    "            if framework_name == \"All results\":\n",
    "                complete_html += process_fn\n",
    "            else:\n",
    "                complete_html += process_fn\n",
    "\n",
    "        # return template.render(\n",
    "        #     table_name=framework_name,\n",
    "        #     table_data=complete_html,\n",
    "        # )\n",
    "        return f\"\"\"<div class=\"container\">\n",
    "                <h2>{framework_name}</h2>\n",
    "                <div class=\"table-responsive\">\n",
    "                    <table\n",
    "                    class=\"table table-striped table-bordered table-hover\"\n",
    "                    style=\"margin: 0 auto; height: 30%; overflow-x: auto; width: 60%\"\n",
    "                    >\n",
    "                    {complete_html}\n",
    "                    </table>\n",
    "                </div>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "\n",
    "    def generate_dashboard_section(self):\n",
    "        dashboard_html = f\"\"\"\n",
    "        <div style=\"text-align: center; margin-bottom: 20px; margin-top: 20px;\">\n",
    "            <h1>Framework Performance Dashboard</h1>\n",
    "        </div>\n",
    "        <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-bottom: 40px;\">\n",
    "        {self.graph_and_heading(self.current_results, self.best_metric.upper() + \"-task\", \"task\", \"result\", \"framework\", f\"{self.best_metric.upper()} of each Framework\", \"1\", \"This is a plot of the main metric used in the experiment against the result of the experiment for each framework for each task. Use this plot to compare the performance of each framework for each task.\", \"bar\")}\n",
    "        {self.graph_and_heading(self.current_results, \"predict-duration-task\", \"framework\", \"predict_duration\", \"framework\", \"Predict Duration of each Framework\", \"2\", \"This is a plot of the prediction duration for each framework for each task. Use this plot to find the framework with the fastest prediction time.\", \"bar\")}\n",
    "        {self.graph_and_heading(self.current_results, \"framework-performance\", \"framework\", \"result\", \"framework\", \"Performance of each Framework\", \"1\", \"This is a plot of the performance of each framework for each task. Use this plot find the best framework for the tasks.\", \"bar\")}\n",
    "        {self.graph_and_heading(self.current_results, \"predict-duration-performance\", \"predict_duration\", \"result\", \"framework\", \"Predict Duration vs Performance\", \"2\", \"This is a scatter plot of the prediction duration against the performance of each framework for each task. Use this plot to find the best framework for the tasks.\", \"scatter\")}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        return dashboard_html\n",
    "\n",
    "    def graph_and_heading(\n",
    "        self,\n",
    "        df,\n",
    "        graph_id,\n",
    "        x,\n",
    "        y,\n",
    "        color,\n",
    "        title,\n",
    "        grid_column,\n",
    "        description,\n",
    "        plot_type=\"bar\",\n",
    "    ):\n",
    "        try:\n",
    "            # Create the plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            if plot_type == \"bar\":\n",
    "                sns.barplot(data=df, x=x, y=y, hue=color, palette=\"muted\")\n",
    "\n",
    "            elif plot_type == \"scatter\":\n",
    "                sns.scatterplot(data=df, x=x, y=y, hue=color, palette=\"muted\")\n",
    "            plt.title(title)\n",
    "            # display values on top of the bars\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save the plot to a buffer\n",
    "            buffer = io.BytesIO()\n",
    "            plt.savefig(buffer, format=\"png\")\n",
    "            buffer.seek(0)\n",
    "            encoded_image = base64.b64encode(buffer.read()).decode()\n",
    "            buffer.close()\n",
    "            plt.close()\n",
    "\n",
    "            # Embed the plot in HTML\n",
    "            graph_html = f\"\"\"\n",
    "            <div style=\"grid-column: {grid_column};\">\n",
    "                <h3 style=\"text-align: center;\">{title}</h3>\n",
    "                <p>{description}</p>\n",
    "                <img src=\"data:image/png;base64,{encoded_image}\" style=\"width: 100%;\">\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            return graph_html\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return f\"<div style='grid-column: {grid_column};'><p>Error generating graph: {str(e)}</p></div>\"\n",
    "\n",
    "    def get_explanation_from_llm(self):\n",
    "        prompt_format = f\"\"\"For a dataset called {self.task_name} , the best framework is {self.best_framework} with a {self.best_metric} of {self.best_result_for_metric}. This is a {self.type_of_task} task. The results are as follows {self.metric_and_result}. For each metric, tell me if this is a good score (and why), and if it is not, how can I improve it? Keep your answer to the point.\n",
    "        The dataset description is: {self.description}\n",
    "    \"\"\"\n",
    "        response: ChatResponse = chat(\n",
    "            model=\"llama3.2\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt_format,\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        return response[\"message\"][\"content\"].replace(\"\\n\", \"<br>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_66553/1148816269.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(\"dataset_description\", axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def run_for_single_dataset(dataset_id):\n",
    "    result_for_dataset = GetResultForDataset(dataset_id)\n",
    "    best_result_table = result_for_dataset.generate_best_result_table()\n",
    "\n",
    "    framework_table = result_for_dataset.generate_framework_table()\n",
    "    dashboard_section = result_for_dataset.generate_dashboard_section()\n",
    "    explanation = result_for_dataset.get_explanation_from_llm()\n",
    "\n",
    "    combined_html = f\"\"\"\n",
    "    <!-- Latest compiled and minified CSS -->\n",
    "<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css\">\n",
    "\n",
    "<!-- jQuery library -->\n",
    "<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js\"></script>\n",
    "\n",
    "<!-- Latest compiled JavaScript -->\n",
    "<script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js\"></script>\n",
    "    <div class=\"container\">\n",
    "        {best_result_table}\n",
    "        {dashboard_section}\n",
    "        <h2>Explanation</h2>\n",
    "        <p>{explanation}</p>\n",
    "        {framework_table}\n",
    "\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    return combined_html\n",
    "\n",
    "\n",
    "rn = run_for_single_dataset(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "GENERATED_REPORTS_DIR = Path(\"./generated_reports\")\n",
    "GENERATED_REPORTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Helper: Find max existing dataset ID\n",
    "def find_max_existing_dataset_id()->int:\n",
    "    conn = sqlite3.connect(\"./data/runs.db\")\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT DISTINCT dataset_id FROM runs\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    return max([x[0] for x in rows]) if rows else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_existing_dataset_id:int = find_max_existing_dataset_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_66553/1148816269.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(\"dataset_description\", axis=1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating report for dataset 1: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [00:08<00:38,  4.32s/it]/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_66553/1148816269.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(\"dataset_description\", axis=1, inplace=True)\n",
      " 27%|██▋       | 3/11 [00:18<00:53,  6.74s/it]/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_66553/1148816269.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(\"dataset_description\", axis=1, inplace=True)\n",
      " 36%|███▋      | 4/11 [00:26<00:48,  6.99s/it]/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_66553/1148816269.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(\"dataset_description\", axis=1, inplace=True)\n",
      " 45%|████▌     | 5/11 [00:39<00:54,  9.05s/it]/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_66553/1148816269.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(\"dataset_description\", axis=1, inplace=True)\n",
      " 55%|█████▍    | 6/11 [00:46<00:42,  8.54s/it]/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_66553/1148816269.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(\"dataset_description\", axis=1, inplace=True)\n",
      " 64%|██████▎   | 7/11 [00:57<00:36,  9.16s/it]/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_66553/1148816269.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(\"dataset_description\", axis=1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating report for dataset 8: expected str, bytes or os.PathLike object, not NoneType\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 9/11 [01:05<00:13,  6.76s/it]/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_66553/1148816269.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(\"dataset_description\", axis=1, inplace=True)\n",
      " 91%|█████████ | 10/11 [01:49<00:16, 16.38s/it]/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_66553/1148816269.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(\"dataset_description\", axis=1, inplace=True)\n",
      "100%|██████████| 11/11 [01:58<00:00, 10.75s/it]\n"
     ]
    }
   ],
   "source": [
    "for dataset_id in tqdm(range(1,max_existing_dataset_id + 1)):\n",
    "    try:\n",
    "        html = run_for_single_dataset(dataset_id)\n",
    "        with open(GENERATED_REPORTS_DIR / f\"report_{dataset_id}.html\", \"w\") as f:\n",
    "            f.write(html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating report for dataset {dataset_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openml.tasks.get_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "openml.config.apikey = openml.config.get_config_as_dict()['apikey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = openml.tasks.get_task(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>product-type</th>\n",
       "      <th>steel</th>\n",
       "      <th>carbon</th>\n",
       "      <th>hardness</th>\n",
       "      <th>temper_rolling</th>\n",
       "      <th>condition</th>\n",
       "      <th>formability</th>\n",
       "      <th>strength</th>\n",
       "      <th>non-ageing</th>\n",
       "      <th>...</th>\n",
       "      <th>s</th>\n",
       "      <th>p</th>\n",
       "      <th>shape</th>\n",
       "      <th>thick</th>\n",
       "      <th>width</th>\n",
       "      <th>len</th>\n",
       "      <th>oil</th>\n",
       "      <th>bore</th>\n",
       "      <th>packing</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COIL</td>\n",
       "      <td>0.700</td>\n",
       "      <td>610.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>R</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COIL</td>\n",
       "      <td>3.200</td>\n",
       "      <td>610.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>R</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SHEET</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COIL</td>\n",
       "      <td>2.801</td>\n",
       "      <td>385.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SHEET</td>\n",
       "      <td>0.801</td>\n",
       "      <td>255.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  family product-type steel  carbon  hardness temper_rolling condition  \\\n",
       "0    NaN            C     A     8.0       0.0            NaN         S   \n",
       "1    NaN            C     R     0.0       0.0            NaN         S   \n",
       "2    NaN            C     R     0.0       0.0            NaN         S   \n",
       "3    NaN            C     A     0.0      60.0              T       NaN   \n",
       "4    NaN            C     A     0.0      60.0              T       NaN   \n",
       "\n",
       "  formability  strength non-ageing  ...    s    p  shape  thick   width  \\\n",
       "0         NaN       0.0        NaN  ...  NaN  NaN   COIL  0.700   610.0   \n",
       "1           2       0.0        NaN  ...  NaN  NaN   COIL  3.200   610.0   \n",
       "2           2       0.0        NaN  ...  NaN  NaN  SHEET  0.700  1300.0   \n",
       "3         NaN       0.0        NaN  ...  NaN  NaN   COIL  2.801   385.1   \n",
       "4         NaN       0.0        NaN  ...  NaN  NaN  SHEET  0.801   255.0   \n",
       "\n",
       "     len  oil bore packing class  \n",
       "0    0.0  NaN    0     NaN     3  \n",
       "1    0.0  NaN    0     NaN     3  \n",
       "2  762.0  NaN    0     NaN     3  \n",
       "3    0.0  NaN    0     NaN     3  \n",
       "4  269.0  NaN    0     NaN     3  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = t.get_dataset().get_data()[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', 'U', '1', '5', '2']\n",
       "Categories (6, object): ['1' < '2' < '3' < '4' < '5' < 'U']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"class\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = openml.datasets.get_dataset(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import CategoricalDtype\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_target_col_type(dataset, target_col_name):\n",
    "    try:\n",
    "        if dataset.features:\n",
    "            return next((feature.data_type for feature in dataset.features.values() if feature.name == target_col_name), None)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "def check_if_api_key_is_valid():\n",
    "    if not openml.config.get_config_as_dict()['apikey']:\n",
    "        print(\"API key is not set. Please set the API key using openml.config.apikey = 'your-key'\")\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def try_create_task(dataset_id):\n",
    "    try:\n",
    "        dataset = openml.datasets.get_dataset(dataset_id)\n",
    "        target_col_name = dataset.default_target_attribute\n",
    "        target_col_type = get_target_col_type(dataset, target_col_name)\n",
    "\n",
    "        if target_col_type:\n",
    "            if target_col_type in ['nominal', 'string', 'categorical']:\n",
    "                evaluation_measure=\"predictive_accuracy\"\n",
    "                task_type = openml.tasks.TaskType.SUPERVISED_CLASSIFICATION\n",
    "            elif target_col_type == 'numeric':\n",
    "                evaluation_measure=\"mean_absolute_error\"\n",
    "                task_type = openml.tasks.TaskType.SUPERVISED_REGRESSION\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "            task = openml.tasks.create_task(\n",
    "                dataset_id=dataset_id,\n",
    "                task_type=task_type,\n",
    "                target_name=target_col_name,\n",
    "                evaluation_measure=evaluation_measure,\n",
    "                estimation_procedure_id=1)\n",
    "            # try:\n",
    "            if check_if_api_key_is_valid():\n",
    "                task.publish()\n",
    "            else:\n",
    "                return None\n",
    "            print(f\"Task created: {task}, task_id: {task.task_id}\")\n",
    "            return task.task_id\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task created: OpenML Classification Task\n",
      "==========================\n",
      "Task Type Description: https://www.openml.org/tt/TaskType.SUPERVISED_CLASSIFICATION\n",
      "Task ID..............: 362130\n",
      "Task URL.............: https://www.openml.org/t/362130\n",
      "Estimation Procedure.: None\n",
      "Evaluation Measure...: predictive_accuracy\n",
      "Target Feature.......: result\n",
      "Cost Matrix..........: Available, task_id: 362130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "362130"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_create_task(46342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- todo\n",
    "  - requirements to poetry\n",
    "  - install all required frameworks\n",
    "  - check if results exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database and delete rows that have framework==autogluon\n",
    "conn = sqlite3.connect('./data/runs.db')\n",
    "c = conn.cursor()\n",
    "benchmarks_to_use = [ \"autoweka\", \"decisiontree\", \"flaml\", \"gama\", \"h20automl\", \"hyperoptsklearn\", \"lightautoml\", \"oboe\", \"tpot\", \"autogluon\"]\n",
    "for framework in benchmarks_to_use:\n",
    "    c.execute(f\"DELETE FROM runs WHERE framework='{framework}'\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "# ['dataset_id', 'task_id', 'framework']\n",
    "conn = sqlite3.connect('./data/runs.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"SELECT distinct dataset_id FROM runs\")\n",
    "rows = c.fetchall()\n",
    "rows = [x[0] for x in rows]\n",
    "max(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "attempt to write a readonly database",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     c\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM runs WHERE task_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m AND dataset_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m AND framework=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframework\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(c\u001b[38;5;241m.\u001b[39mfetchall())\n\u001b[0;32m---> 28\u001b[0m \u001b[43madd_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mautogluon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36madd_run\u001b[0;34m(task_id, dataset_id, framework)\u001b[0m\n\u001b[1;32m     19\u001b[0m conn \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/runs.db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m c \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mINSERT INTO runs VALUES (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtask_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mframework\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m conn\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[1;32m     23\u001b[0m conn \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/runs.db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mOperationalError\u001b[0m: attempt to write a readonly database"
     ]
    }
   ],
   "source": [
    "# conn = sqlite3.connect('./data/runs.db')\n",
    "# c = conn.cursor()\n",
    "\n",
    "# # add task (10, 10, 'autogluon') to the database\n",
    "# c.execute(\"INSERT INTO runs VALUES (10, 10, 'autogluon')\")\n",
    "# conn.commit()\n",
    "\n",
    "# load the db as writable\n",
    "conn = sqlite3.connect('./data/runs.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "# check if the task (10, 10, 'autogluon') is in the database\n",
    "c.execute(\"SELECT * FROM runs WHERE task_id=10 AND dataset_id=10 AND framework='autogluon'\")\n",
    "print(c.fetchall())\n",
    "\n",
    "# write a function that takes a task_id and dataset_id and framework and adds it to the database\n",
    "\n",
    "def add_run(task_id, dataset_id, framework):\n",
    "    conn = sqlite3.connect('./data/runs.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute(f\"INSERT INTO runs VALUES ({task_id}, {dataset_id}, '{framework}')\")\n",
    "    conn.commit()\n",
    "    conn = sqlite3.connect('./data/runs.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute(f\"SELECT * FROM runs WHERE task_id={task_id} AND dataset_id={dataset_id} AND framework='{framework}'\")\n",
    "    print(c.fetchall())\n",
    "\n",
    "add_run(10, 10, 'autogluon')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 2, 'randomforest'), (3, 3, 'randomforest'), (4, 4, 'randomforest'), (5, 5, 'randomforest'), (6, 6, 'randomforest'), (7, 7, 'randomforest'), (8, 8, 'randomforest'), (9, 9, 'randomforest'), (10, 10, 'randomforest'), (11, 11, 'randomforest'), (2, 2, 'autosklearn'), (3, 3, 'autosklearn'), (4, 4, 'autosklearn'), (5, 5, 'autosklearn'), (6, 6, 'autosklearn'), (7, 7, 'autosklearn'), (8, 8, 'autosklearn'), (9, 9, 'autosklearn'), (10, 10, 'autosklearn'), (11, 11, 'autosklearn')]\n"
     ]
    }
   ],
   "source": [
    "# select * from runs\n",
    "c.execute(\"SELECT * FROM runs\")\n",
    "print(c.fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenML Dataset\n",
       "==============\n",
       "Name..........: anneal\n",
       "Version.......: 2\n",
       "Format........: ARFF\n",
       "Upload Date...: 2014-04-06 23:19:20\n",
       "Licence.......: Public\n",
       "Download URL..: https://api.openml.org/data/v1/download/1/anneal.arff\n",
       "OpenML URL....: https://www.openml.org/d/1\n",
       "# of features.: 39\n",
       "# of instances: 898"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openml.datasets.get_dataset(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smukherjee/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm.auto import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskConfig:\n",
    "    def __init__(self, *args):\n",
    "        self.args = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskFinder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        testing_mode=True,\n",
    "        use_cache=True,\n",
    "        run_mode=\"docker\",\n",
    "        num_tasks_to_return=1,\n",
    "        save_every_n_tasks=10,\n",
    "    ):\n",
    "        self.testing_mode = testing_mode\n",
    "        self.cache_file_name = \"data/dataset_list.csv\"\n",
    "        self.global_results_store = {}\n",
    "        self.num_tasks_to_return = num_tasks_to_return\n",
    "        self.use_cache = use_cache\n",
    "        self.save_every_n_tasks = save_every_n_tasks\n",
    "        # self.benchmarks_to_use = [\"autosklearn\"]\n",
    "        self.benchmarks_to_use = [\"randomforest\", \"autogluon\"]\n",
    "        # Ensure required folders exist\n",
    "        self.make_files([\"data\"])\n",
    "        self.run_mode = run_mode\n",
    "\n",
    "        self.check_run_mode()\n",
    "        # Load datasets from cache or OpenML\n",
    "        self.datasets = self.load_datasets()\n",
    "\n",
    "        # If in testing mode, only take the first dataset\n",
    "        if self.testing_mode:\n",
    "            # self.datasets = self.datasets.head(1)\n",
    "            # 43972 - tic tac\n",
    "            self.datasets = self.datasets[self.datasets[\"did\"] == 50]\n",
    "\n",
    "    def check_run_mode(self):\n",
    "        possible = [\"local\", \"aws\", \"docker\", \"singularity\"]\n",
    "        if self.run_mode not in possible:\n",
    "            raise ValueError(\n",
    "                f\"Invalid run mode: {self.run_mode}. Possible values are: {possible}\"\n",
    "            )\n",
    "\n",
    "    def load_datasets(self):\n",
    "        datasets = openml.datasets.list_datasets(output_format=\"dataframe\")\n",
    "        # if the cache file exists, append the new unique datasets to it\n",
    "        if self.use_cache and os.path.exists(self.cache_file_name):\n",
    "            cached_datasets = pd.read_csv(self.cache_file_name)\n",
    "        else:\n",
    "            if os.path.exists(self.cache_file_name):\n",
    "                cached_datasets = pd.read_csv(self.cache_file_name)\n",
    "                datasets = pd.concat([datasets, cached_datasets], ignore_index=True)\n",
    "                datasets = datasets.drop_duplicates(subset=[\"did\"])\n",
    "\n",
    "                # Save the updated dataset list to the cache\n",
    "                datasets.to_csv(self.cache_file_name)\n",
    "        return datasets\n",
    "\n",
    "    def make_files(self, folders):\n",
    "        for folder in folders:\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    def get_tasks_from_dataset(self, dataset_id, num_tasks_to_return=1):\n",
    "        try:\n",
    "            tasks = openml.tasks.list_tasks(\n",
    "                data_id=dataset_id, output_format=\"dataframe\"\n",
    "            )\n",
    "            # if the task column estimation_procedure is not 10-fold Crossvalidation drop the row\n",
    "            tasks = tasks[tasks[\"estimation_procedure\"] == \"10-fold Crossvalidation\"]\n",
    "            # return the first num_tasks_to_return tasks\n",
    "            tasks = tasks.head(num_tasks_to_return)\n",
    "            return tasks[\"tid\"].tolist() if not len(tasks) == 0 else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving tasks for dataset {dataset_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def run_all_benchmarks_on_task(self, task_id):\n",
    "        # task_id = task_id.strip()\n",
    "        for benchmark_type in self.benchmarks_to_use:\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\n",
    "                        \"python3\",\n",
    "                        \"automlbenchmark/runbenchmark.py\",\n",
    "                        benchmark_type,\n",
    "                        f\"openml/t/{task_id}\",\n",
    "                        \"--mode\",\n",
    "                        self.run_mode,\n",
    "                    ],\n",
    "                    text=True,\n",
    "                    capture_output=True,\n",
    "                )\n",
    "                print(result.stderr)\n",
    "                # Filter and return relevant benchmark output\n",
    "                if self.run_mode == \"local\":\n",
    "                    return [\n",
    "                        line for line in result.stderr.split(\"\\n\") if \"TaskConfig\" in line\n",
    "                    ]\n",
    "                elif self.run_mode == \"docker\":\n",
    "                   return [\n",
    "                        line for line in result.stderr.split(\"\\n\") if \"Starting docker: docker run --name \" in line\n",
    "                    ] \n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Benchmark run failed for task {task_id}: {e}\")\n",
    "                return []\n",
    "\n",
    "    def get_task_config_from_str(self, task_str):\n",
    "        try:\n",
    "            if self.run_mode == \"local\":\n",
    "                return eval(task_str).__dict__[\"args\"][0]\n",
    "            elif self.run_mode == \"docker\":\n",
    "                # --name randomforest.openml_t_49.test.docker.20241009T153843.sIiURqJG8Z99apxsQC8ISg__ \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing task config: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_task_for_dataset(self, dataset_id):\n",
    "        task_ids = self.get_tasks_from_dataset(dataset_id, self.num_tasks_to_return)\n",
    "        if task_ids:\n",
    "            for task_id in tqdm(\n",
    "                task_ids, desc=f\"Running benchmark on dataset {dataset_id}\"\n",
    "            ):\n",
    "                benchmark_results = self.run_all_benchmarks_on_task(\n",
    "                    task_id,\n",
    "                )\n",
    "                for result in tqdm(\n",
    "                    benchmark_results, desc=f\"Processing results for task {task_id}\"\n",
    "                ):\n",
    "                    task_config = self.get_task_config_from_str(result)\n",
    "                    if task_config and self.run_mode == \"local\":\n",
    "                        current_run_info = {\n",
    "                            \"dataset_id\": dataset_id,\n",
    "                            \"task_id\": task_id,\n",
    "                            \"task_config\": task_config,\n",
    "                        }\n",
    "                        self.global_results_store[dataset_id] = current_run_info\n",
    "                    if task_config and self.run_mode == \"docker\":\n",
    "                        current_run_info = {\n",
    "                            \"dataset_id\": dataset_id,\n",
    "                            \"task_id\": task_id,\n",
    "                        }\n",
    "                # write the results to a file every 10 tasks\n",
    "                if len(self.global_results_store) % self.save_every_n_tasks == 0:\n",
    "                    self.write_global_to_file()\n",
    "\n",
    "    def write_global_to_file(\n",
    "        self, file_name=\"data/links_to_automl_files_per_dataset.json\"\n",
    "    ):\n",
    "        # read the current file if it exists and append the new results\n",
    "        if os.path.exists(file_name):\n",
    "            with open(file_name, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                data.update(self.global_results_store)\n",
    "        else:\n",
    "            data = self.global_results_store\n",
    "        with open(file_name, \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "    def load_global_from_file(\n",
    "        self, file_name=\"data/links_to_automl_files_per_dataset.json\"\n",
    "    ):\n",
    "        if os.path.exists(file_name):\n",
    "            with open(file_name, \"r\") as f:\n",
    "                self.global_results_store = json.load(f)\n",
    "        else:\n",
    "            print(\"No global results file found\")\n",
    "\n",
    "    def run_benchmark_on_all_datasets(self):\n",
    "        for _, row in self.datasets.iterrows():\n",
    "            dataset_id = row[\"did\"]\n",
    "            self.get_task_for_dataset(dataset_id)\n",
    "\n",
    "        # save info\n",
    "        self.write_global_to_file()\n",
    "\n",
    "    def upload_results_to_openml(self):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TaskFinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running benchmark on dataset 50:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/smukherjee/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "Running benchmark `randomforest` on `openml/t/49` framework in `docker` mode.\n",
      "Loading frameworks definitions from ['/Users/smukherjee/Documents/CODE/Github/OpenML-auto-automl/automlbenchmark/resources/frameworks.yaml'].\n",
      "Loading benchmark constraint definitions from ['/Users/smukherjee/Documents/CODE/Github/OpenML-auto-automl/automlbenchmark/resources/constraints.yaml'].\n",
      "Loading openml task 49.\n",
      "Running cmd `docker images -q automlbenchmark/randomforest:stable-dev`\n",
      "Running cmd `docker pull automlbenchmark/randomforest:stable-dev`\n",
      "Error response from daemon: manifest for automlbenchmark/randomforest:stable-dev not found: manifest unknown: manifest unknown\n",
      "\n",
      "\n",
      "\n",
      "Error response from daemon: manifest for automlbenchmark/randomforest:stable-dev not found: manifest unknown: manifest unknown\n",
      "\n",
      "Running cmd `docker images -q automlbenchmark/randomforest:stable`\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] CPU Utilization: 26.1%\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Starting job docker.openml_t_49.test.all_tasks.all_folds.RandomForest.\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] Memory Usage: 63.4%\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] Disk Usage: 6.5%\n",
      "Starting docker: docker run --name randomforest.openml_t_49.test.docker.20241009T153843.sIiURqJG8Z99apxsQC8ISg__ --shm-size=2048M  -v /Users/smukherjee/.openml:/input -v /Users/smukherjee/Documents/CODE/Github/OpenML-auto-automl/automlbenchmark/results/randomforest.openml_t_49.test.docker.20241009T153843:/output -v /Users/smukherjee/.config/automlbenchmark:/custom --rm automlbenchmark/randomforest:stable randomforest openml/t/49 test   -Xseed=auto -i /input -o /output -u /custom -s skip -Xrun_mode=docker --session=.\n",
      "Datasets are loaded by default from folder /Users/smukherjee/.openml.\n",
      "Generated files will be available in folder /Users/smukherjee/Documents/CODE/Github/OpenML-auto-automl/automlbenchmark/results.\n",
      "Running cmd `docker run --name randomforest.openml_t_49.test.docker.20241009T153843.sIiURqJG8Z99apxsQC8ISg__ --shm-size=2048M  -v /Users/smukherjee/.openml:/input -v /Users/smukherjee/Documents/CODE/Github/OpenML-auto-automl/automlbenchmark/results/randomforest.openml_t_49.test.docker.20241009T153843:/output -v /Users/smukherjee/.config/automlbenchmark:/custom --rm automlbenchmark/randomforest:stable randomforest openml/t/49 test   -Xseed=auto -i /input -o /output -u /custom -s skip -Xrun_mode=docker --session=`\n",
      "WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n",
      "Running benchmark `randomforest` on `openml/t/49` framework in `local` mode.\n",
      "Loading frameworks definitions from ['/bench/resources/frameworks.yaml'].\n",
      "Loading benchmark constraint definitions from ['/bench/resources/constraints.yaml'].\n",
      "Loading openml task 49.\n",
      "[MONITORING] [python3.7 [1]] CPU Utilization: 18.4%\n",
      "[MONITORING] [python3.7 [1]] Memory Usage: 5.0%\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Starting job local.openml_t_49.test.tic-tac-toe.0.RandomForest.\n",
      "[MONITORING] [python3.7 [1]] Disk Usage: 76.1%\n",
      "Assigning 4 cores (total=11) for new task tic-tac-toe.\n",
      "Assigning 10744 MB (total=13462 MB) for new tic-tac-toe task.\n",
      "Running task tic-tac-toe on framework RandomForest with config:\n",
      "TaskConfig({})\n",
      "Running cmd `/bench/frameworks/RandomForest/venv/bin/python -W ignore /bench/frameworks/RandomForest/exec.py`\n",
      "INFO:exec.py:\n",
      "**** Random Forest [sklearn v1.0] ****\n",
      "\n",
      "\n",
      "\n",
      "INFO:exec.py:Running RandomForest with a maximum time of 600s on 4 cores.\n",
      "\n",
      "WARNING:exec.py:We completely ignore the requirement to stay within the time limit.\n",
      "\n",
      "WARNING:exec.py:We completely ignore the advice to optimize towards metric: auc.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predictions preview:\n",
      "     negative  positive predictions     truth\n",
      "0     0.1260    0.8740    positive  positive\n",
      "1     0.2070    0.7930    positive  positive\n",
      "2     0.1700    0.8300    positive  positive\n",
      "3     0.1685    0.8315    positive  positive\n",
      "4     0.1560    0.8440    positive  positive\n",
      "5     0.0540    0.9460    positive  positive\n",
      "6     0.0550    0.9450    positive  positive\n",
      "7     0.0750    0.9250    positive  positive\n",
      "8     0.0875    0.9125    positive  positive\n",
      "9     0.0280    0.9720    positive  positive\n",
      "10    0.0795    0.9205    positive  positive\n",
      "11    0.0495    0.9505    positive  positive\n",
      "12    0.3390    0.6610    positive  positive\n",
      "13    0.1830    0.8170    positive  positive\n",
      "14    0.0735    0.9265    positive  positive\n",
      "15    0.1705    0.8295    positive  positive\n",
      "16    0.0370    0.9630    positive  positive\n",
      "17    0.1180    0.8820    positive  positive\n",
      "18    0.1245    0.8755    positive  positive\n",
      "19    0.1870    0.8130    positive  positive\n",
      "\n",
      "Predictions saved to `/output/predictions/tic-tac-toe/0/predictions.csv`.\n",
      "Loading metadata from `/output/predictions/tic-tac-toe/0/metadata.json`.\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "\n",
      "Loading predictions from `/output/predictions/tic-tac-toe/0/predictions.csv`.\n",
      "Metric scores: { 'acc': 0.96875,\n",
      "  'app_version': 'dev [NA, NA, NA]',\n",
      "  'auc': 1.0,\n",
      "  'balacc': 0.9545454545454546,\n",
      "  'constraint': 'test',\n",
      "  'duration': nan,\n",
      "  'fold': 0,\n",
      "  'framework': 'RandomForest',\n",
      "  'id': 'openml.org/t/49',\n",
      "  'info': None,\n",
      "  'logloss': 0.21935486367102217,\n",
      "  'metric': 'auc',\n",
      "  'mode': 'docker',\n",
      "  'models_count': 2000,\n",
      "  'params': \"{'n_estimators': 2000}\",\n",
      "  'predict_duration': 1.0324931144714355,\n",
      "  'result': 1.0,\n",
      "  'seed': 1475303368,\n",
      "  'task': 'tic-tac-toe',\n",
      "  'training_duration': 11.24776291847229,\n",
      "  'type': 'binary',\n",
      "  'utc': '2024-10-09T15:39:06',\n",
      "  'version': '1.0'}\n",
      "Job `local.openml_t_49.test.tic-tac-toe.0.RandomForest` executed in 16.887 seconds.\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Starting job local.openml_t_49.test.tic-tac-toe.1.RandomForest.\n",
      "Assigning 4 cores (total=11) for new task tic-tac-toe.\n",
      "Assigning 10737 MB (total=13462 MB) for new tic-tac-toe task.\n",
      "Running task tic-tac-toe on framework RandomForest with config:\n",
      "TaskConfig({})\n",
      "Running cmd `/bench/frameworks/RandomForest/venv/bin/python -W ignore /bench/frameworks/RandomForest/exec.py`\n",
      "INFO:exec.py:\n",
      "**** Random Forest [sklearn v1.0] ****\n",
      "\n",
      "INFO:exec.py:Running RandomForest with a maximum time of 600s on 4 cores.\n",
      "WARNING:exec.py:We completely ignore the requirement to stay within the time limit.\n",
      "WARNING:exec.py:We completely ignore the advice to optimize towards metric: auc.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predictions preview:\n",
      "     negative  positive predictions     truth\n",
      "0     0.0900    0.9100    positive  positive\n",
      "1     0.0370    0.9630    positive  positive\n",
      "2     0.0415    0.9585    positive  positive\n",
      "3     0.2045    0.7955    positive  positive\n",
      "4     0.1070    0.8930    positive  positive\n",
      "5     0.2105    0.7895    positive  positive\n",
      "6     0.0835    0.9165    positive  positive\n",
      "7     0.0250    0.9750    positive  positive\n",
      "8     0.0400    0.9600    positive  positive\n",
      "9     0.1900    0.8100    positive  positive\n",
      "10    0.2285    0.7715    positive  positive\n",
      "11    0.3385    0.6615    positive  positive\n",
      "12    0.1625    0.8375    positive  positive\n",
      "13    0.0335    0.9665    positive  positive\n",
      "14    0.0835    0.9165    positive  positive\n",
      "15    0.1415    0.8585    positive  positive\n",
      "16    0.0455    0.9545    positive  positive\n",
      "17    0.0470    0.9530    positive  positive\n",
      "18    0.0235    0.9765    positive  positive\n",
      "19    0.1290    0.8710    positive  positive\n",
      "\n",
      "Predictions saved to `/output/predictions/tic-tac-toe/1/predictions.csv`.\n",
      "Loading metadata from `/output/predictions/tic-tac-toe/1/metadata.json`.\n",
      "Loading predictions from `/output/predictions/tic-tac-toe/1/predictions.csv`.\n",
      "Metric scores: { 'acc': 0.96875,\n",
      "  'app_version': 'dev [NA, NA, NA]',\n",
      "  'auc': 1.0,\n",
      "  'balacc': 0.9545454545454546,\n",
      "  'constraint': 'test',\n",
      "  'duration': nan,\n",
      "  'fold': 1,\n",
      "  'framework': 'RandomForest',\n",
      "  'id': 'openml.org/t/49',\n",
      "  'info': None,\n",
      "  'logloss': 0.20959471547170616,\n",
      "  'metric': 'auc',\n",
      "  'mode': 'docker',\n",
      "  'models_count': 2000,\n",
      "  'params': \"{'n_estimators': 2000}\",\n",
      "  'predict_duration': 1.0233509540557861,\n",
      "  'result': 1.0,\n",
      "  'seed': 1475303369,\n",
      "  'task': 'tic-tac-toe',\n",
      "  'training_duration': 11.208200693130493,\n",
      "  'type': 'binary',\n",
      "  'utc': '2024-10-09T15:39:23',\n",
      "  'version': '1.0'}\n",
      "Job `local.openml_t_49.test.tic-tac-toe.1.RandomForest` executed in 16.766 seconds.\n",
      "All jobs executed in 33.697 seconds.\n",
      "[MONITORING] [python3.7 [1]] CPU Utilization: 10.6%\n",
      "[MONITORING] [python3.7 [1]] Memory Usage: 5.2%\n",
      "[MONITORING] [python3.7 [1]] Disk Usage: 76.1%\n",
      "Processing results for \n",
      "Scores saved to `/output/scores/RandomForest.benchmark_openml_t_49.csv`.\n",
      "Scores saved to `/output/scores/results.csv`.\n",
      "Scores saved to `/output/results.csv`.\n",
      "Summing up scores for current run:\n",
      "             id        task fold    framework constraint  result metric  duration       seed\n",
      "openml.org/t/49 tic-tac-toe    0 RandomForest       test     1.0    auc      16.9 1475303368\n",
      "openml.org/t/49 tic-tac-toe    1 RandomForest       test     1.0    auc      16.8 1475303369\n",
      "Job `docker.openml_t_49.test.all_tasks.all_folds.RandomForest` executed in 38.700 seconds.\n",
      "All jobs executed in 38.702 seconds.\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] CPU Utilization: 28.1%\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] Memory Usage: 62.4%\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] Disk Usage: 6.5%\n",
      "Summing up scores for current run:\n",
      "             id        task  fold    framework constraint  result metric  duration       seed\n",
      "openml.org/t/49 tic-tac-toe     0 RandomForest       test     1.0    auc      16.9 1475303368\n",
      "openml.org/t/49 tic-tac-toe     1 RandomForest       test     1.0    auc      16.8 1475303369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing results for task 49: 100%|██████████| 2/2 [00:00<00:00, 18157.16it/s]\n",
      "Running benchmark on dataset 50: 100%|██████████| 1/1 [00:41<00:00, 41.49s/it]\n"
     ]
    }
   ],
   "source": [
    "tf.run_benchmark_on_all_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 2\u001b[0m store \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mglobal_results_store\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m datasets \u001b[38;5;129;01min\u001b[39;00m store\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      4\u001b[0m     result_file \u001b[38;5;241m=\u001b[39m Path(store[datasets][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_config\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores/results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "store = tf.global_results_store\n",
    "for datasets in store.keys():\n",
    "    result_file = Path(store[datasets][\"task_config\"][\"output_dir\"])/\"scores/results.csv\"\n",
    "    pandas_file = pd.read_csv(result_file)\n",
    "    print(pandas_file.head())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'task', 'framework', 'constraint', 'fold', 'type', 'result',\n",
       "       'metric', 'mode', 'version', 'params', 'app_version', 'utc', 'duration',\n",
       "       'training_duration', 'predict_duration', 'models_count', 'seed', 'info',\n",
       "       'acc', 'auc', 'balacc', 'logloss'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_file.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'framework': 'RandomForest', 'framework_params': {'n_estimators': 2000}, 'framework_version': '1.0', 'type': 'classification', 'name': 'anneal', 'fold': 1, 'metric': 'logloss', 'metrics': ['logloss', 'acc', 'balacc'], 'seed': 1095784781, 'job_timeout_seconds': 1200, 'max_runtime_seconds': 600, 'cores': 4, 'max_mem_size_mb': 10721, 'min_vol_size_mb': -1, 'input_dir': '/input', 'output_dir': '/output/', 'output_predictions_file': '/output/predictions/anneal/1/predictions.csv', 'ext': {}, 'type_': 'multiclass', 'output_metadata_file': '/output/predictions/anneal/1/metadata.json'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x330402d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Dash app\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "# Layout\n",
    "app.layout = dbc.Container([\n",
    "    dbc.Row([\n",
    "        dbc.Col(html.H1(\"Model Results Dashboard\"), width=12)\n",
    "    ]),\n",
    "    \n",
    "    dbc.Row([\n",
    "        dbc.Col([\n",
    "            html.Label(\"Select Metric:\"),\n",
    "            dcc.Dropdown(\n",
    "                id='metric-dropdown',\n",
    "                options=[\n",
    "                    {'label': 'Accuracy', 'value': 'acc'},\n",
    "                    {'label': 'AUC', 'value': 'auc'},\n",
    "                    {'label': 'Log Loss', 'value': 'logloss'}\n",
    "                ],\n",
    "                value='acc',\n",
    "                clearable=False\n",
    "            )\n",
    "        ], width=4),\n",
    "    ]),\n",
    "\n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Graph(id='metric-graph'), width=12)\n",
    "    ]),\n",
    "\n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Graph(id='task-pie-chart'), width=6),\n",
    "        dbc.Col(dcc.Graph(id='framework-bar-chart'), width=6),\n",
    "    ]),\n",
    "\n",
    "], fluid=True)\n",
    "\n",
    "# Callbacks\n",
    "@app.callback(\n",
    "    Output('metric-graph', 'figure'),\n",
    "    Output('task-pie-chart', 'figure'),\n",
    "    Output('framework-bar-chart', 'figure'),\n",
    "    Input('metric-dropdown', 'value')\n",
    ")\n",
    "\n",
    "\n",
    "# Run app and display result inline in the notebook\n",
    "app.run_server(mode='inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_task_id_from_folder_name(folder_name):\n",
    "    name = re.findall(pattern=r\"_t_.*[0-9]\\.\", string=folder_name)\n",
    "    print(name)\n",
    "    if isinstance(name, list) and len(name) > 0:\n",
    "        return str(name[0])\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_t_5.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'_t_5.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_task_id_from_folder_name(folder_name=\"randomforest.openml_t_5.test.docker.20241016T141320\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = openml.tasks.get_task(10, download_data=False, download_qualities= False)\n",
    "t.dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automlb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
