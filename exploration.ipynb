{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dash import dcc, html\n",
    "from flask import Flask, render_template, jsonify, request, send_file\n",
    "from glob import glob\n",
    "from interpret import set_visualize_provider\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from interpret.provider import InlineProvider\n",
    "from itables import to_html_datatable\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from ollama import ChatResponse\n",
    "from ollama import chat\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional, Union\n",
    "from utils import OpenMLTaskHandler\n",
    "import base64\n",
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "import io\n",
    "import json\n",
    "import markdown\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openml\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import re\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from typing import Any\n",
    "matplotlib.use('agg')\n",
    "set_visualize_provider(InlineProvider())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_load_file(file_path, file_type) -> Union[pd.DataFrame, dict, None]:\n",
    "        \"\"\"\n",
    "        This function is responsible for safely loading a file. It returns None if the file is not found or if there is an error loading the file.\n",
    "        \"\"\"\n",
    "        if file_type == \"json\":\n",
    "            try:\n",
    "                with open(str(Path(file_path)), \"r\") as f:\n",
    "                    return json.load(f)\n",
    "            except:\n",
    "                return None\n",
    "        elif file_type == \"pd\":\n",
    "            try:\n",
    "                return pd.read_csv(str(file_path))\n",
    "            except:\n",
    "                return None\n",
    "        elif file_type == \"textdict\":\n",
    "            try:\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    return json.loads(f.read())\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_existing_dataset_id()->int:\n",
    "    conn = sqlite3.connect(\"./data/runs.db\")\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT DISTINCT dataset_id FROM runs\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    return max([x[0] for x in rows]) if rows else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReportGenerator:\n",
    "    def __init__(self, generated_ebm_report_dir):\n",
    "        self.generated_ebm_report_dir = generated_ebm_report_dir\n",
    "\n",
    "    def generate_ebm_report(self, names, scores):\n",
    "        fig = px.bar(\n",
    "            x=names,\n",
    "            y=scores,\n",
    "            orientation='v',\n",
    "            color_discrete_sequence=px.colors.qualitative.Safe\n",
    "        )\n",
    "        fig.update_layout(\n",
    "            title=\"Feature Importance\",\n",
    "            xaxis_title=\"Feature\",\n",
    "            yaxis_title=\"Score\"\n",
    "        )\n",
    "        return fig.to_html(full_html=False, include_plotlyjs='cdn')\n",
    "\n",
    "    def run_ebm_on_dataset(self, dataset_id, X_train, y_train):\n",
    "        try:\n",
    "            ebm = ExplainableBoostingClassifier(random_state=42)\n",
    "            ebm.fit(X_train, y_train)\n",
    "            ebm_global = ebm.explain_global().data()\n",
    "            names, scores = ebm_global[\"names\"], ebm_global[\"scores\"]\n",
    "            return self.generate_ebm_report(names, scores)\n",
    "        except Exception as e:\n",
    "            return \"<div>Unable to generate feature importance report</div>\"\n",
    "\n",
    "    def get_data_and_split(self, dataset_id):\n",
    "        dataset = openml.datasets.get_dataset(dataset_id=dataset_id)\n",
    "        X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
    "        X = pd.get_dummies(X, prefix_sep='.').astype(float)\n",
    "        y, y_categories = y.factorize()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "        return X, y, X_train, y_train\n",
    "\n",
    "    \n",
    "    def get_feature_distribution(self, X):\n",
    "        try:\n",
    "            return to_html_datatable(X.describe().T)\n",
    "        except Exception as e:\n",
    "            return \"<div>Unable to generate feature distribution</div>\"\n",
    "\n",
    "    def class_imbalance(self, y):\n",
    "        try:\n",
    "            return to_html_datatable(pd.DataFrame(y).value_counts(), index=True)\n",
    "        except Exception as e:\n",
    "            return \"<div>Unable to generate class imbalance report</div>\"\n",
    "        \n",
    "    def get_missing_value_count(self, X):\n",
    "        try:\n",
    "            return to_html_datatable(pd.DataFrame(X.isnull().sum(), columns=[\"Missing Value Count\"]))\n",
    "        except Exception as e:\n",
    "            return \"<div>Unable to generate missing value count</div>\"\n",
    "\n",
    "    def generate_data_report_for_dataset(self, dataset_id):\n",
    "        report_path = f\"{self.generated_ebm_report_dir}/{dataset_id}_report.html\"\n",
    "        if os.path.exists(report_path):\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                X, y, X_train, y_train = self.get_data_and_split(dataset_id)\n",
    "\n",
    "                ebm_report = self.run_ebm_on_dataset(dataset_id, X_train, y_train)\n",
    "                missing_value_count = self.get_missing_value_count(X)\n",
    "                feature_distribution = self.get_feature_distribution(X)\n",
    "                class_imbalance_report = self.class_imbalance(y)\n",
    "\n",
    "                report_html = f\"\"\"\n",
    "                    <h1>Extra Dataset Information</h1>\n",
    "                    \n",
    "                    <h2>Feature Importance</h2>\n",
    "                    {ebm_report}\n",
    "                    <h2>Feature Distribution</h2>\n",
    "                    {feature_distribution}\n",
    "                    <h2>Class Imbalance</h2>\n",
    "                    {class_imbalance_report}\n",
    "                    <h2>Missing Value Count</h2>\n",
    "                    {missing_value_count}\n",
    "                    </div>\n",
    "                    \"\"\"\n",
    "                \n",
    "                report_path = f\"{self.generated_ebm_report_dir}/{dataset_id}_report.html\"\n",
    "                with open(report_path, \"w\") as f:\n",
    "                    f.write(report_html)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing dataset {dataset_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultCollector:\n",
    "    def __init__(self, path: str = \"./data/results/*\"):\n",
    "        self.experiment_directory = Path(path)\n",
    "\n",
    "        self.all_run_paths = glob(pathname=str(self.experiment_directory))\n",
    "        self.all_results = pd.DataFrame()\n",
    "        self.openml_task_handler = OpenMLTaskHandler()\n",
    "        # Required columns\n",
    "        self.required_columns = {\n",
    "            \"metric\",\n",
    "            \"result\",\n",
    "            \"framework\",\n",
    "            \"dataset_id\",\n",
    "            \"id\",\n",
    "            \"task\",\n",
    "            \"predict_duration\",\n",
    "            \"models\",\n",
    "        }\n",
    "\n",
    "        # Define how to find the best result for the metric\n",
    "        self.metric_used_dict = {\n",
    "            \"auc\": lambda x: x.max(),\n",
    "            \"neg_logloss\": lambda x: x.min(),\n",
    "        }\n",
    "    \n",
    "    def get_dataset_description_from_id(self, dataset_id: int) -> Optional[str]:\n",
    "        return openml.datasets.get_dataset(dataset_id).description\n",
    "\n",
    "    def collect_all_run_info_to_df(self):\n",
    "        \"\"\"\n",
    "        This function is responsible for loading all the results files from the runs and storing them in self.all_results. This is further used to generate the dashboard.\n",
    "        \"\"\"\n",
    "        all_results_list = []  # Temporary list to store individual DataFrames\n",
    "\n",
    "        for run_path in tqdm(self.all_run_paths, total=len(self.all_run_paths)):\n",
    "            run_path = Path(run_path)\n",
    "            results_file_path = run_path / \"results.csv\"\n",
    "\n",
    "            # Load results file if it exists\n",
    "            results_file = safe_load_file(results_file_path, \"pd\")\n",
    "\n",
    "            # If results file is loaded, proceed to process it\n",
    "            if results_file is not None:\n",
    "                # Get the model path specific to this run_path\n",
    "                models_path_list = list((run_path / \"models\").rglob(\"models.*\"))\n",
    "                leaderboard_path_list = list(\n",
    "                    (run_path / \"models\").rglob(\"leaderboard.*\")\n",
    "                )\n",
    "                # models_path = str(models_path_list[0]) if len(models_path_list) >0 else None\n",
    "\n",
    "                if len(models_path_list) > 0:\n",
    "                    models_path = str(models_path_list[0])\n",
    "                elif len(leaderboard_path_list) > 0:\n",
    "                    models_path = str(leaderboard_path_list[0])\n",
    "                else:\n",
    "                    models_path = None\n",
    "\n",
    "                # Add the model path as a new column in the current results_file DataFrame\n",
    "                results_file[\"models\"] = models_path\n",
    "\n",
    "                # Get the dataset ID for each row in the results file\n",
    "                results_file[\"dataset_id\"] = results_file[\"id\"].apply(\n",
    "                    self.openml_task_handler.get_dataset_id_from_task_id\n",
    "                )\n",
    "                results_file[\"dataset_description\"] = results_file[\"dataset_id\"].apply(\n",
    "                    self.get_dataset_description_from_id\n",
    "                )\n",
    "\n",
    "                # Append the processed DataFrame to our list\n",
    "                all_results_list.append(results_file)\n",
    "\n",
    "        # Concatenate all individual DataFrames into self.all_results\n",
    "        if all_results_list:\n",
    "            self.all_results = pd.concat(all_results_list, ignore_index=True)\n",
    "    \n",
    "    def validate_dataframe_and_add_extra_info(self):\n",
    "        # Validate DataFrame\n",
    "        if self.all_results is None or self.all_results.empty:\n",
    "            return \"Error: Provided DataFrame is empty or None.\"\n",
    "\n",
    "        # Handle duplicate frameworks by keeping the one with the best result\n",
    "        self.all_results = self.all_results.drop_duplicates(subset=[\"framework\"], keep=\"first\")\n",
    "\n",
    "        # Add missing columns with default values\n",
    "        for column in self.required_columns:\n",
    "            if column not in self.all_results.columns:\n",
    "                self.all_results[column] = \"N/A\"\n",
    "        \n",
    "    def __call__(self):\n",
    "        self.collect_all_run_info_to_df()\n",
    "        return self.all_results\n",
    "        # self.validate_dataframe_and_add_extra_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateCompleteReportForDataset:\n",
    "    def __init__(self, dataset_id: int, collector_results, GENERATED_REPORTS_DIR: str = \"./data/generated_reports\", GENERATED_DATA_REPORT_DIR: str = \"./data/generated_data_reports\"):\n",
    "        self.dataset_id = dataset_id\n",
    "        self.collector_results = collector_results\n",
    "        self.current_results = self.get_results_for_dataset_id(self.dataset_id)\n",
    "        self.jinja_environment = Environment(\n",
    "            loader=FileSystemLoader(\"./website_assets/templates/\")\n",
    "        )\n",
    "        self.generated_final_reports_dir = GENERATED_REPORTS_DIR\n",
    "        self.generated_data_reports_dir = GENERATED_DATA_REPORT_DIR\n",
    "        self.template_to_use = {\n",
    "            \"best_result\": \"best_result_table.html\",\n",
    "            \"framework_table\": \"framework_table.html\",\n",
    "            \"metric_vs_result\": \"metric_vs_result.html\",\n",
    "        }\n",
    "        binary_metrics = [\n",
    "            \"auc\",\n",
    "            \"logloss\",\n",
    "            \"acc\",\n",
    "            \"balacc\",\n",
    "        ]  # available metrics: auc (AUC), acc (Accuracy), balacc (Balanced Accuracy), pr_auc (Precision Recall AUC), logloss (Log Loss), f1, f2, f05 (F-beta scores with beta=1, 2, or 0.5), max_pce, mean_pce (Max/Mean Per-Class Error).\n",
    "        multiclass_metrics = [\n",
    "            \"logloss\",\n",
    "            \"acc\",\n",
    "            \"balacc\",\n",
    "        ]  # available metrics: same as for binary, except auc, replaced by auc_ovo (AUC One-vs-One), auc_ovr (AUC One-vs-Rest). AUC metrics and F-beta metrics are computed with weighted average.\n",
    "        regression_metrics = [\n",
    "            \"rmse\",\n",
    "            \"r2\",\n",
    "            \"mae\",\n",
    "        ]  # available metrics: mae (Mean Absolute Error), mse (Mean Squared Error), msle (Mean Squared Logarithmic Error), rmse (Root Mean Square Error), rmsle (Root Mean Square Logarithmic Error), r2 (R^2).\n",
    "        timeseries_metrics = [\n",
    "            \"mase\",\n",
    "            \"mape\",\n",
    "            \"smape\",\n",
    "            \"wape\",\n",
    "            \"rmse\",\n",
    "            \"mse\",\n",
    "            \"mql\",\n",
    "            \"wql\",\n",
    "            \"sql\",\n",
    "        ]  # available metrics: mase (Mean Absolute Scaled Error), mape (Mean Absolute Percentage Error),\n",
    "        self.all_metrics = (\n",
    "            binary_metrics\n",
    "            + multiclass_metrics\n",
    "            + regression_metrics\n",
    "            + timeseries_metrics\n",
    "        )\n",
    "\n",
    "        # run the function to get the best result\n",
    "        self.framework_names = [\"Auto-sklearn\", \"H20AutoML\", \"AutoGluon\", \"All results\"]\n",
    "        self.process_fns = [\n",
    "            self.process_auto_sklearn_data(self.current_results),\n",
    "            self.get_rows_for_framework_from_df(\n",
    "                df=self.current_results, framework_name=\"H20AutoML\", top_n=10\n",
    "            ),\n",
    "            self.get_rows_for_framework_from_df(\n",
    "                df=self.current_results, framework_name=\"AutoGluon\", top_n=10\n",
    "            ),\n",
    "            self.get_rows_for_framework_from_df(\n",
    "                df=self.current_results, framework_name=\"All results\"\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        self.best_framework = \"\"\n",
    "        self.best_metric = \"\"\n",
    "        self.type_of_task =\"\"\n",
    "        self.dataset_id = \"\"\n",
    "        self.task_id = \"\"\n",
    "        self.task_name = \"\"\n",
    "        self.best_result_for_metric = \"\"\n",
    "        self.description = \"\"\n",
    "        self.metric_and_result = \"\"\n",
    "\n",
    "        self.get_best_result()\n",
    "    def get_results_for_dataset_id(self, dataset_id: int) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        This function returns the results for a given dataset_id. If no results are found, it returns None.\n",
    "        \"\"\"\n",
    "        results_for_dataset = self.collector_results[\n",
    "            self.collector_results[\"dataset_id\"] == dataset_id\n",
    "        ]\n",
    "        if results_for_dataset.empty:\n",
    "            return None\n",
    "        return results_for_dataset\n",
    "\n",
    "\n",
    "    def get_best_result(self):\n",
    "        \"\"\"\n",
    "        This function returns the best result from the current_results DataFrame. It first sorts the DataFrame based on the metric used and then returns the best result.\n",
    "        \"\"\"\n",
    "        if self.current_results is None:\n",
    "            return None\n",
    "        metric_used = self.current_results[\"metric\"].iloc[0]\n",
    "        if metric_used in [\"auc\", \"acc\", \"balacc\"]:\n",
    "            # Since higher value is better we sort in descending order\n",
    "            sort_in_ascending_order = False\n",
    "        elif metric_used in [\"logloss\", \"neg_logloss\"]:\n",
    "            # Since lower value is better we sort in ascending order\n",
    "            sort_in_ascending_order = True\n",
    "        else:\n",
    "            sort_in_ascending_order = False\n",
    "\n",
    "        sorted_results = self.current_results.sort_values(\n",
    "            by=\"result\", ascending=sort_in_ascending_order\n",
    "        ).head()\n",
    "\n",
    "        best_result = sorted_results.iloc[0]\n",
    "        self.best_framework = best_result.get(\"framework\", \"\")\n",
    "        self.best_metric = best_result.get(\"metric\", \"\")\n",
    "        self.type_of_task = best_result.get(\"type\", \"\")\n",
    "        self.dataset_id = best_result.get(\"dataset_id\", \"\")\n",
    "        self.task_id = \"https://\" + best_result.get(\"id\", \"\")\n",
    "        self.task_name = best_result.get(\"task\", \"\")\n",
    "        self.best_result_for_metric = best_result.get(\"result\", \"\")\n",
    "        self.description = best_result.get(\"dataset_description\", \"\")\n",
    "\n",
    "        # all metric columns that are in the dataframe and in the list of all metrics\n",
    "        metric_columns = [\n",
    "            col for col in self.current_results.columns if col in self.all_metrics\n",
    "        ]\n",
    "        all_metrics_present = []\n",
    "        for metric in metric_columns:\n",
    "            try:\n",
    "                all_metrics_present.append(self.current_results[metric].values[0])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        self.metric_and_result = \" \".join(\n",
    "            [\n",
    "                f\"The {metric} is {result} \"\n",
    "                for metric, result in zip(metric_columns, all_metrics_present)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def generate_best_result_table(self):\n",
    "        \"\"\"\n",
    "        This function generates the best result table using the best result information.\n",
    "        \"\"\"\n",
    "        template = self.jinja_environment.get_template(\n",
    "            self.template_to_use[\"best_result\"]\n",
    "        )\n",
    "        return template.render(\n",
    "            best_framework=self.best_framework,\n",
    "            best_metric=self.best_metric,\n",
    "            type_of_task=self.type_of_task,\n",
    "            dataset_id=self.dataset_id,\n",
    "            task_id=self.task_id,\n",
    "            task_name=self.task_name,\n",
    "        )\n",
    "\n",
    "    def process_auto_sklearn_data(self, df, top_n=10):\n",
    "        auto_sklearn_data = pd.DataFrame()\n",
    "        try:\n",
    "            auto_sklearn_rows = df[df[\"framework\"] == \"autosklearn\"]\n",
    "            # for each row, read the json file from the models column and get the model id and cost\n",
    "            for _, row in auto_sklearn_rows.iterrows():\n",
    "                models_path = row[\"models\"]\n",
    "                try:\n",
    "                    with open(models_path, \"r\") as f:\n",
    "                        models_file = json.load(f)\n",
    "                        for model in models_file:\n",
    "                            model_type = (\n",
    "                                \"sklearn_classifier\"\n",
    "                                if \"sklearn_classifier\" in models_file[model]\n",
    "                                else \"sklearn_regressor\"\n",
    "                            )\n",
    "\n",
    "                            auto_sklearn_data = pd.concat(\n",
    "                                [auto_sklearn_data, pd.DataFrame([models_file[model]])],\n",
    "                                ignore_index=True,\n",
    "                            )\n",
    "                except:\n",
    "                    pass\n",
    "                auto_sklearn_data = auto_sklearn_data.sort_values(\n",
    "                    by=\"cost\", ascending=True\n",
    "                ).head(top_n)\n",
    "                return to_html_datatable(auto_sklearn_data, caption=\"Auto Sklearn Models\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return \"<div></div>\"\n",
    "\n",
    "        # return auto_sklearn_data.to_html()\n",
    "\n",
    "    def get_rows_for_framework_from_df(\n",
    "        self, df: pd.DataFrame, framework_name, top_n=40\n",
    "    ):\n",
    "        try:\n",
    "            if framework_name == \"All results\":\n",
    "                # drop the description column if it exists\n",
    "                try:\n",
    "                    df.drop(\"dataset_description\", axis=1)\n",
    "                except:\n",
    "                    pass\n",
    "                return to_html_datatable(df, caption=\"All Results\")\n",
    "            framework_rows: pd.DataFrame = df[df[\"framework\"] == framework_name][\n",
    "                \"models\"\n",
    "            ].values[0]\n",
    "            framework_data = safe_load_file(framework_rows, \"pd\")\n",
    "            if top_n is not None:\n",
    "                framework_data = framework_data.head(40)\n",
    "\n",
    "            return to_html_datatable(framework_data, caption=f\"{framework_name} Models\")\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    def generate_framework_table(self):\n",
    "        \"\"\"\n",
    "        This function generates the framework table using the framework_name information.\n",
    "        \"\"\"\n",
    "        complete_html = \"\"\n",
    "        for framework_name, process_fn in zip(self.framework_names, self.process_fns):\n",
    "            try:\n",
    "                complete_html += process_fn\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        return f\"\"\"<div class=\"container\">\n",
    "                <h2>{framework_name}</h2>\n",
    "                    {complete_html}\n",
    "                </div>\n",
    "                \"\"\"\n",
    "\n",
    "    def generate_dashboard_section(self):\n",
    "        dashboard_html = f\"\"\"\n",
    "        <div style=\"text-align: center; margin-bottom: 20px; margin-top: 20px;\">\n",
    "            <h1>Framework Performance Dashboard</h1>\n",
    "        </div>\n",
    "        <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-bottom: 40px;\">\n",
    "        {self.graph_and_heading(self.current_results, self.best_metric.upper() + \"-task\", \"task\", \"result\", \"framework\", f\"{self.best_metric.upper()} of each Framework\", \"1\", \"This is a plot of the main metric used in the experiment against the result of the experiment for each framework for each task. Use this plot to compare the performance of each framework for each task.\", \"bar\")}\n",
    "        {self.graph_and_heading(self.current_results, \"predict-duration-task\", \"framework\", \"predict_duration\", \"framework\", \"Predict Duration of each Framework\", \"2\", \"This is a plot of the prediction duration for each framework for each task. Use this plot to find the framework with the fastest prediction time.\", \"bar\")}\n",
    "        {self.graph_and_heading(self.current_results, \"framework-performance\", \"framework\", \"result\", \"framework\", \"Performance of each Framework\", \"1\", \"This is a plot of the performance of each framework for each task. Use this plot find the best framework for the tasks.\", \"bar\")}\n",
    "        {self.graph_and_heading(self.current_results, \"predict-duration-performance\", \"predict_duration\", \"result\", \"framework\", \"Predict Duration vs Performance\", \"2\", \"This is a scatter plot of the prediction duration against the performance of each framework for each task. Use this plot to find the best framework for the tasks.\", \"scatter\")}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        return dashboard_html\n",
    "\n",
    "    def graph_and_heading(\n",
    "        self,\n",
    "        df,\n",
    "        graph_id,\n",
    "        x,\n",
    "        y,\n",
    "        color,\n",
    "        title,\n",
    "        grid_column,\n",
    "        description,\n",
    "        plot_type=\"bar\",\n",
    "    ):\n",
    "        try:\n",
    "            colors = px.colors.qualitative.Safe\n",
    "            if len(x) == 0:\n",
    "                return \"<div></div>\"\n",
    "\n",
    "            # use plotly to create the plot\n",
    "            if plot_type == \"bar\":\n",
    "                fig = px.bar(df, x=x, y=y, color=color, title=title, color_discrete_sequence=colors)\n",
    "            elif plot_type == \"scatter\":\n",
    "                fig = px.scatter(df, x=x, y=y, color=color, title=title, color_discrete_sequence=colors)\n",
    "\n",
    "            fig.update_layout(\n",
    "                title=title,\n",
    "                xaxis_title=x,\n",
    "                yaxis_title=y,\n",
    "            )\n",
    "            encoded_image = fig.to_html(full_html=False, include_plotlyjs=\"cdn\")\n",
    "\n",
    "            return f\"<div style='grid-column: {grid_column};'>{encoded_image}</div>\"\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return f\"<div style='grid-column: {grid_column};'><p>Error generating graph: {str(e)}</p></div>\"\n",
    "\n",
    "    def get_explanation_from_llm(self):\n",
    "        prompt_format = f\"\"\"For a dataset called {self.task_name} , the best framework is {self.best_framework} with a {self.best_metric} of {self.best_result_for_metric}. This is a {self.type_of_task} task. The results are as follows {self.metric_and_result}. For each metric, tell me if this is a good score (and why), and if it is not, how can I improve it? Keep your answer to the point.\n",
    "        The dataset description is: {self.description}\n",
    "    \"\"\"\n",
    "        response: ChatResponse = chat(\n",
    "            model=\"llama3.2\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt_format,\n",
    "                },\n",
    "            ],\n",
    "            options={\n",
    "                \"temperature\": 0.3,\n",
    "            }\n",
    "        )\n",
    "        response = response[\"message\"][\"content\"]\n",
    "        markdown_response = markdown.markdown(response)\n",
    "        return markdown_response\n",
    "    \n",
    "    def get_data_report(self):\n",
    "        try:\n",
    "            with open(f\"{self.generated_data_reports_dir}/{self.dataset_id}_report.html\", \"r\") as f:\n",
    "                return f.read()\n",
    "        except FileNotFoundError:\n",
    "            return \"<div><p>Feature importance not available for this dataset</p></div>\"\n",
    "    \n",
    "    def __call__(self):\n",
    "        best_result_table = self.generate_best_result_table()\n",
    "        framework_table = self.generate_framework_table()\n",
    "        dashboard_section = self.generate_dashboard_section()\n",
    "        explanation = self.get_explanation_from_llm()\n",
    "        feature_importance = self.get_data_report()\n",
    "        combined_html = f\"\"\"\n",
    "    <!-- Latest compiled and minified CSS -->\n",
    "<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css\">\n",
    "\n",
    "<!-- jQuery library -->\n",
    "<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js\"></script>\n",
    "\n",
    "<!-- Latest compiled JavaScript -->\n",
    "<script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js\"></script>\n",
    "    <div class=\"container\">\n",
    "        {best_result_table}\n",
    "        {feature_importance}\n",
    "        {dashboard_section}\n",
    "        <div>\n",
    "        <h1>Explanation and What's next?</h1>\n",
    "        <p>!!! This is an AI-generated (llama3.2) explanation of the results. Please take the response with a grain of salt and use your own judgement.</p>\n",
    "        <p>{explanation}</p>\n",
    "        </div>\n",
    "        {framework_table}\n",
    "\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "        with open(Path(self.generated_final_reports_dir)/f\"report_{self.dataset_id}.html\", \"w\") as f:\n",
    "            f.write(combined_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "GENERATED_DATA_REPORT_DIR = Path(\"./data/generated_data_reports\")\n",
    "os.makedirs(GENERATED_DATA_REPORT_DIR, exist_ok=True)\n",
    "\n",
    "GENERATED_REPORTS_DIR = Path(\"./data/generated_reports\")\n",
    "GENERATED_REPORTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# find the largest dataset id that has been processed\n",
    "max_existing_dataset_id:int = find_max_existing_dataset_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_report_script_for_all_datasets(GENERATED_DATA_REPORT_DIR, GENERATED_REPORTS_DIR, max_existing_dataset_id):\n",
    "    # collect all the results from the runs\n",
    "    collector = ResultCollector()\n",
    "    all_results = collector()\n",
    "    drg = DataReportGenerator(GENERATED_DATA_REPORT_DIR)\n",
    "    for dataset_id in tqdm(range(1,max_existing_dataset_id + 1)):\n",
    "        try:\n",
    "            # generate the data report for all datasets\n",
    "            drg.generate_data_report_for_dataset(dataset_id=dataset_id)\n",
    "            # write complete report to a file\n",
    "            GenerateCompleteReportForDataset(dataset_id=dataset_id,collector_results=all_results, GENERATED_DATA_REPORT_DIR=GENERATED_DATA_REPORT_DIR, GENERATED_REPORTS_DIR=GENERATED_REPORTS_DIR)()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating report for dataset {dataset_id}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:00<00:00, 394.66it/s]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating report for dataset 1: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3/11 [00:18<00:48,  6.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_report_script_for_all_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGENERATED_DATA_REPORT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGENERATED_REPORTS_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_existing_dataset_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[63], line 11\u001b[0m, in \u001b[0;36mrun_report_script_for_all_datasets\u001b[0;34m(GENERATED_DATA_REPORT_DIR, GENERATED_REPORTS_DIR, max_existing_dataset_id)\u001b[0m\n\u001b[1;32m      9\u001b[0m     drg\u001b[38;5;241m.\u001b[39mgenerate_data_report_for_dataset(dataset_id\u001b[38;5;241m=\u001b[39mdataset_id)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# write complete report to a file\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mGenerateCompleteReportForDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcollector_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGENERATED_DATA_REPORT_DIR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGENERATED_DATA_REPORT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGENERATED_REPORTS_DIR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGENERATED_REPORTS_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError generating report for dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[61], line 294\u001b[0m, in \u001b[0;36mGenerateCompleteReportForDataset.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m         framework_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_framework_table()\n\u001b[1;32m    293\u001b[0m         dashboard_section \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_dashboard_section()\n\u001b[0;32m--> 294\u001b[0m         explanation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_explanation_from_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m         feature_importance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_data_report()\n\u001b[1;32m    296\u001b[0m         combined_html \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124m    <!-- Latest compiled and minified CSS -->\u001b[39m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124m<link rel=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstylesheet\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m href=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124m        </div>\u001b[39m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[61], line 267\u001b[0m, in \u001b[0;36mGenerateCompleteReportForDataset.get_explanation_from_llm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_explanation_from_llm\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    264\u001b[0m     prompt_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mFor a dataset called \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m , the best framework is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_framework\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_result_for_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_of_task\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m task. The results are as follows \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_and_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For each metric, tell me if this is a good score (and why), and if it is not, how can I improve it? Keep your answer to the point.\u001b[39m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124m    The dataset description is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 267\u001b[0m     response: ChatResponse \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3.2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     response \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    280\u001b[0m     markdown_response \u001b[38;5;241m=\u001b[39m markdown\u001b[38;5;241m.\u001b[39mmarkdown(response)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/ollama/_client.py:331\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[1;32m    287\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    288\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[1;32m    297\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/ollama/_client.py:175\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[1;32m    173\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/ollama/_client.py:116\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 116\u001b[0m   r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/httpx/_client.py:837\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    822\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    824\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    825\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    826\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    835\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    836\u001b[0m )\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_report_script_for_all_datasets(GENERATED_DATA_REPORT_DIR, GENERATED_REPORTS_DIR, max_existing_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openml.tasks.get_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "openml.config.apikey = openml.config.get_config_as_dict()['apikey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = openml.tasks.get_task(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>product-type</th>\n",
       "      <th>steel</th>\n",
       "      <th>carbon</th>\n",
       "      <th>hardness</th>\n",
       "      <th>temper_rolling</th>\n",
       "      <th>condition</th>\n",
       "      <th>formability</th>\n",
       "      <th>strength</th>\n",
       "      <th>non-ageing</th>\n",
       "      <th>...</th>\n",
       "      <th>s</th>\n",
       "      <th>p</th>\n",
       "      <th>shape</th>\n",
       "      <th>thick</th>\n",
       "      <th>width</th>\n",
       "      <th>len</th>\n",
       "      <th>oil</th>\n",
       "      <th>bore</th>\n",
       "      <th>packing</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COIL</td>\n",
       "      <td>0.700</td>\n",
       "      <td>610.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>R</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COIL</td>\n",
       "      <td>3.200</td>\n",
       "      <td>610.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>R</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SHEET</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COIL</td>\n",
       "      <td>2.801</td>\n",
       "      <td>385.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SHEET</td>\n",
       "      <td>0.801</td>\n",
       "      <td>255.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  family product-type steel  carbon  hardness temper_rolling condition  \\\n",
       "0    NaN            C     A     8.0       0.0            NaN         S   \n",
       "1    NaN            C     R     0.0       0.0            NaN         S   \n",
       "2    NaN            C     R     0.0       0.0            NaN         S   \n",
       "3    NaN            C     A     0.0      60.0              T       NaN   \n",
       "4    NaN            C     A     0.0      60.0              T       NaN   \n",
       "\n",
       "  formability  strength non-ageing  ...    s    p  shape  thick   width  \\\n",
       "0         NaN       0.0        NaN  ...  NaN  NaN   COIL  0.700   610.0   \n",
       "1           2       0.0        NaN  ...  NaN  NaN   COIL  3.200   610.0   \n",
       "2           2       0.0        NaN  ...  NaN  NaN  SHEET  0.700  1300.0   \n",
       "3         NaN       0.0        NaN  ...  NaN  NaN   COIL  2.801   385.1   \n",
       "4         NaN       0.0        NaN  ...  NaN  NaN  SHEET  0.801   255.0   \n",
       "\n",
       "     len  oil bore packing class  \n",
       "0    0.0  NaN    0     NaN     3  \n",
       "1    0.0  NaN    0     NaN     3  \n",
       "2  762.0  NaN    0     NaN     3  \n",
       "3    0.0  NaN    0     NaN     3  \n",
       "4  269.0  NaN    0     NaN     3  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = t.get_dataset().get_data()[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', 'U', '1', '5', '2']\n",
       "Categories (6, object): ['1' < '2' < '3' < '4' < '5' < 'U']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"class\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = openml.datasets.get_dataset(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import CategoricalDtype\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_target_col_type(dataset, target_col_name):\n",
    "    try:\n",
    "        if dataset.features:\n",
    "            return next((feature.data_type for feature in dataset.features.values() if feature.name == target_col_name), None)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "def check_if_api_key_is_valid():\n",
    "    if not openml.config.get_config_as_dict()['apikey']:\n",
    "        print(\"API key is not set. Please set the API key using openml.config.apikey = 'your-key'\")\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def try_create_task(dataset_id):\n",
    "    try:\n",
    "        dataset = openml.datasets.get_dataset(dataset_id)\n",
    "        target_col_name = dataset.default_target_attribute\n",
    "        target_col_type = get_target_col_type(dataset, target_col_name)\n",
    "\n",
    "        if target_col_type:\n",
    "            if target_col_type in ['nominal', 'string', 'categorical']:\n",
    "                evaluation_measure=\"predictive_accuracy\"\n",
    "                task_type = openml.tasks.TaskType.SUPERVISED_CLASSIFICATION\n",
    "            elif target_col_type == 'numeric':\n",
    "                evaluation_measure=\"mean_absolute_error\"\n",
    "                task_type = openml.tasks.TaskType.SUPERVISED_REGRESSION\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "            task = openml.tasks.create_task(\n",
    "                dataset_id=dataset_id,\n",
    "                task_type=task_type,\n",
    "                target_name=target_col_name,\n",
    "                evaluation_measure=evaluation_measure,\n",
    "                estimation_procedure_id=1)\n",
    "            # try:\n",
    "            if check_if_api_key_is_valid():\n",
    "                task.publish()\n",
    "            else:\n",
    "                return None\n",
    "            print(f\"Task created: {task}, task_id: {task.task_id}\")\n",
    "            return task.task_id\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task created: OpenML Classification Task\n",
      "==========================\n",
      "Task Type Description: https://www.openml.org/tt/TaskType.SUPERVISED_CLASSIFICATION\n",
      "Task ID..............: 362130\n",
      "Task URL.............: https://www.openml.org/t/362130\n",
      "Estimation Procedure.: None\n",
      "Evaluation Measure...: predictive_accuracy\n",
      "Target Feature.......: result\n",
      "Cost Matrix..........: Available, task_id: 362130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "362130"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_create_task(46342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- todo\n",
    "  - requirements to poetry\n",
    "  - install all required frameworks\n",
    "  - check if results exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database and delete rows that have framework==autogluon\n",
    "conn = sqlite3.connect('./data/runs.db')\n",
    "c = conn.cursor()\n",
    "benchmarks_to_use = [ \"autoweka\", \"decisiontree\", \"flaml\", \"gama\", \"h20automl\", \"hyperoptsklearn\", \"lightautoml\", \"oboe\", \"tpot\", \"autogluon\"]\n",
    "for framework in benchmarks_to_use:\n",
    "    c.execute(f\"DELETE FROM runs WHERE framework='{framework}'\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "# ['dataset_id', 'task_id', 'framework']\n",
    "conn = sqlite3.connect('./data/runs.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"SELECT distinct dataset_id FROM runs\")\n",
    "rows = c.fetchall()\n",
    "rows = [x[0] for x in rows]\n",
    "max(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "attempt to write a readonly database",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     c\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM runs WHERE task_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m AND dataset_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m AND framework=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframework\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(c\u001b[38;5;241m.\u001b[39mfetchall())\n\u001b[0;32m---> 28\u001b[0m \u001b[43madd_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mautogluon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36madd_run\u001b[0;34m(task_id, dataset_id, framework)\u001b[0m\n\u001b[1;32m     19\u001b[0m conn \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/runs.db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m c \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mINSERT INTO runs VALUES (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtask_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mframework\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m conn\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[1;32m     23\u001b[0m conn \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/runs.db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mOperationalError\u001b[0m: attempt to write a readonly database"
     ]
    }
   ],
   "source": [
    "# conn = sqlite3.connect('./data/runs.db')\n",
    "# c = conn.cursor()\n",
    "\n",
    "# # add task (10, 10, 'autogluon') to the database\n",
    "# c.execute(\"INSERT INTO runs VALUES (10, 10, 'autogluon')\")\n",
    "# conn.commit()\n",
    "\n",
    "# load the db as writable\n",
    "conn = sqlite3.connect('./data/runs.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "# check if the task (10, 10, 'autogluon') is in the database\n",
    "c.execute(\"SELECT * FROM runs WHERE task_id=10 AND dataset_id=10 AND framework='autogluon'\")\n",
    "print(c.fetchall())\n",
    "\n",
    "# write a function that takes a task_id and dataset_id and framework and adds it to the database\n",
    "\n",
    "def add_run(task_id, dataset_id, framework):\n",
    "    conn = sqlite3.connect('./data/runs.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute(f\"INSERT INTO runs VALUES ({task_id}, {dataset_id}, '{framework}')\")\n",
    "    conn.commit()\n",
    "    conn = sqlite3.connect('./data/runs.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute(f\"SELECT * FROM runs WHERE task_id={task_id} AND dataset_id={dataset_id} AND framework='{framework}'\")\n",
    "    print(c.fetchall())\n",
    "\n",
    "add_run(10, 10, 'autogluon')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 2, 'randomforest'), (3, 3, 'randomforest'), (4, 4, 'randomforest'), (5, 5, 'randomforest'), (6, 6, 'randomforest'), (7, 7, 'randomforest'), (8, 8, 'randomforest'), (9, 9, 'randomforest'), (10, 10, 'randomforest'), (11, 11, 'randomforest'), (2, 2, 'autosklearn'), (3, 3, 'autosklearn'), (4, 4, 'autosklearn'), (5, 5, 'autosklearn'), (6, 6, 'autosklearn'), (7, 7, 'autosklearn'), (8, 8, 'autosklearn'), (9, 9, 'autosklearn'), (10, 10, 'autosklearn'), (11, 11, 'autosklearn')]\n"
     ]
    }
   ],
   "source": [
    "# select * from runs\n",
    "c.execute(\"SELECT * FROM runs\")\n",
    "print(c.fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenML Dataset\n",
       "==============\n",
       "Name..........: anneal\n",
       "Version.......: 2\n",
       "Format........: ARFF\n",
       "Upload Date...: 2014-04-06 23:19:20\n",
       "Licence.......: Public\n",
       "Download URL..: https://api.openml.org/data/v1/download/1/anneal.arff\n",
       "OpenML URL....: https://www.openml.org/d/1\n",
       "# of features.: 39\n",
       "# of instances: 898"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openml.datasets.get_dataset(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smukherjee/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm.auto import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskConfig:\n",
    "    def __init__(self, *args):\n",
    "        self.args = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskFinder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        testing_mode=True,\n",
    "        use_cache=True,\n",
    "        run_mode=\"docker\",\n",
    "        num_tasks_to_return=1,\n",
    "        save_every_n_tasks=10,\n",
    "    ):\n",
    "        self.testing_mode = testing_mode\n",
    "        self.cache_file_name = \"data/dataset_list.csv\"\n",
    "        self.global_results_store = {}\n",
    "        self.num_tasks_to_return = num_tasks_to_return\n",
    "        self.use_cache = use_cache\n",
    "        self.save_every_n_tasks = save_every_n_tasks\n",
    "        # self.benchmarks_to_use = [\"autosklearn\"]\n",
    "        self.benchmarks_to_use = [\"randomforest\", \"autogluon\"]\n",
    "        # Ensure required folders exist\n",
    "        self.make_files([\"data\"])\n",
    "        self.run_mode = run_mode\n",
    "\n",
    "        self.check_run_mode()\n",
    "        # Load datasets from cache or OpenML\n",
    "        self.datasets = self.load_datasets()\n",
    "\n",
    "        # If in testing mode, only take the first dataset\n",
    "        if self.testing_mode:\n",
    "            # self.datasets = self.datasets.head(1)\n",
    "            # 43972 - tic tac\n",
    "            self.datasets = self.datasets[self.datasets[\"did\"] == 50]\n",
    "\n",
    "    def check_run_mode(self):\n",
    "        possible = [\"local\", \"aws\", \"docker\", \"singularity\"]\n",
    "        if self.run_mode not in possible:\n",
    "            raise ValueError(\n",
    "                f\"Invalid run mode: {self.run_mode}. Possible values are: {possible}\"\n",
    "            )\n",
    "\n",
    "    def load_datasets(self):\n",
    "        datasets = openml.datasets.list_datasets(output_format=\"dataframe\")\n",
    "        # if the cache file exists, append the new unique datasets to it\n",
    "        if self.use_cache and os.path.exists(self.cache_file_name):\n",
    "            cached_datasets = pd.read_csv(self.cache_file_name)\n",
    "        else:\n",
    "            if os.path.exists(self.cache_file_name):\n",
    "                cached_datasets = pd.read_csv(self.cache_file_name)\n",
    "                datasets = pd.concat([datasets, cached_datasets], ignore_index=True)\n",
    "                datasets = datasets.drop_duplicates(subset=[\"did\"])\n",
    "\n",
    "                # Save the updated dataset list to the cache\n",
    "                datasets.to_csv(self.cache_file_name)\n",
    "        return datasets\n",
    "\n",
    "    def make_files(self, folders):\n",
    "        for folder in folders:\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    def get_tasks_from_dataset(self, dataset_id, num_tasks_to_return=1):\n",
    "        try:\n",
    "            tasks = openml.tasks.list_tasks(\n",
    "                data_id=dataset_id, output_format=\"dataframe\"\n",
    "            )\n",
    "            # if the task column estimation_procedure is not 10-fold Crossvalidation drop the row\n",
    "            tasks = tasks[tasks[\"estimation_procedure\"] == \"10-fold Crossvalidation\"]\n",
    "            # return the first num_tasks_to_return tasks\n",
    "            tasks = tasks.head(num_tasks_to_return)\n",
    "            return tasks[\"tid\"].tolist() if not len(tasks) == 0 else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving tasks for dataset {dataset_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def run_all_benchmarks_on_task(self, task_id):\n",
    "        # task_id = task_id.strip()\n",
    "        for benchmark_type in self.benchmarks_to_use:\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\n",
    "                        \"python3\",\n",
    "                        \"automlbenchmark/runbenchmark.py\",\n",
    "                        benchmark_type,\n",
    "                        f\"openml/t/{task_id}\",\n",
    "                        \"--mode\",\n",
    "                        self.run_mode,\n",
    "                    ],\n",
    "                    text=True,\n",
    "                    capture_output=True,\n",
    "                )\n",
    "                print(result.stderr)\n",
    "                # Filter and return relevant benchmark output\n",
    "                if self.run_mode == \"local\":\n",
    "                    return [\n",
    "                        line for line in result.stderr.split(\"\\n\") if \"TaskConfig\" in line\n",
    "                    ]\n",
    "                elif self.run_mode == \"docker\":\n",
    "                   return [\n",
    "                        line for line in result.stderr.split(\"\\n\") if \"Starting docker: docker run --name \" in line\n",
    "                    ] \n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Benchmark run failed for task {task_id}: {e}\")\n",
    "                return []\n",
    "\n",
    "    def get_task_config_from_str(self, task_str):\n",
    "        try:\n",
    "            if self.run_mode == \"local\":\n",
    "                return eval(task_str).__dict__[\"args\"][0]\n",
    "            elif self.run_mode == \"docker\":\n",
    "                # --name randomforest.openml_t_49.test.docker.20241009T153843.sIiURqJG8Z99apxsQC8ISg__ \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing task config: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_task_for_dataset(self, dataset_id):\n",
    "        task_ids = self.get_tasks_from_dataset(dataset_id, self.num_tasks_to_return)\n",
    "        if task_ids:\n",
    "            for task_id in tqdm(\n",
    "                task_ids, desc=f\"Running benchmark on dataset {dataset_id}\"\n",
    "            ):\n",
    "                benchmark_results = self.run_all_benchmarks_on_task(\n",
    "                    task_id,\n",
    "                )\n",
    "                for result in tqdm(\n",
    "                    benchmark_results, desc=f\"Processing results for task {task_id}\"\n",
    "                ):\n",
    "                    task_config = self.get_task_config_from_str(result)\n",
    "                    if task_config and self.run_mode == \"local\":\n",
    "                        current_run_info = {\n",
    "                            \"dataset_id\": dataset_id,\n",
    "                            \"task_id\": task_id,\n",
    "                            \"task_config\": task_config,\n",
    "                        }\n",
    "                        self.global_results_store[dataset_id] = current_run_info\n",
    "                    if task_config and self.run_mode == \"docker\":\n",
    "                        current_run_info = {\n",
    "                            \"dataset_id\": dataset_id,\n",
    "                            \"task_id\": task_id,\n",
    "                        }\n",
    "                # write the results to a file every 10 tasks\n",
    "                if len(self.global_results_store) % self.save_every_n_tasks == 0:\n",
    "                    self.write_global_to_file()\n",
    "\n",
    "    def write_global_to_file(\n",
    "        self, file_name=\"data/links_to_automl_files_per_dataset.json\"\n",
    "    ):\n",
    "        # read the current file if it exists and append the new results\n",
    "        if os.path.exists(file_name):\n",
    "            with open(file_name, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                data.update(self.global_results_store)\n",
    "        else:\n",
    "            data = self.global_results_store\n",
    "        with open(file_name, \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "    def load_global_from_file(\n",
    "        self, file_name=\"data/links_to_automl_files_per_dataset.json\"\n",
    "    ):\n",
    "        if os.path.exists(file_name):\n",
    "            with open(file_name, \"r\") as f:\n",
    "                self.global_results_store = json.load(f)\n",
    "        else:\n",
    "            print(\"No global results file found\")\n",
    "\n",
    "    def run_benchmark_on_all_datasets(self):\n",
    "        for _, row in self.datasets.iterrows():\n",
    "            dataset_id = row[\"did\"]\n",
    "            self.get_task_for_dataset(dataset_id)\n",
    "\n",
    "        # save info\n",
    "        self.write_global_to_file()\n",
    "\n",
    "    def upload_results_to_openml(self):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TaskFinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running benchmark on dataset 50:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/smukherjee/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "Running benchmark `randomforest` on `openml/t/49` framework in `docker` mode.\n",
      "Loading frameworks definitions from ['/Users/smukherjee/Documents/CODE/Github/OpenML-auto-automl/automlbenchmark/resources/frameworks.yaml'].\n",
      "Loading benchmark constraint definitions from ['/Users/smukherjee/Documents/CODE/Github/OpenML-auto-automl/automlbenchmark/resources/constraints.yaml'].\n",
      "Loading openml task 49.\n",
      "Running cmd `docker images -q automlbenchmark/randomforest:stable-dev`\n",
      "Running cmd `docker pull automlbenchmark/randomforest:stable-dev`\n",
      "Error response from daemon: manifest for automlbenchmark/randomforest:stable-dev not found: manifest unknown: manifest unknown\n",
      "\n",
      "\n",
      "\n",
      "Error response from daemon: manifest for automlbenchmark/randomforest:stable-dev not found: manifest unknown: manifest unknown\n",
      "\n",
      "Running cmd `docker images -q automlbenchmark/randomforest:stable`\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] CPU Utilization: 26.1%\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Starting job docker.openml_t_49.test.all_tasks.all_folds.RandomForest.\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] Memory Usage: 63.4%\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] Disk Usage: 6.5%\n",
      "Starting docker: docker run --name randomforest.openml_t_49.test.docker.20241009T153843.sIiURqJG8Z99apxsQC8ISg__ --shm-size=2048M  -v /Users/smukherjee/.openml:/input -v /Users/smukherjee/Documents/CODE/Github/OpenML-auto-automl/automlbenchmark/results/randomforest.openml_t_49.test.docker.20241009T153843:/output -v /Users/smukherjee/.config/automlbenchmark:/custom --rm automlbenchmark/randomforest:stable randomforest openml/t/49 test   -Xseed=auto -i /input -o /output -u /custom -s skip -Xrun_mode=docker --session=.\n",
      "Datasets are loaded by default from folder /Users/smukherjee/.openml.\n",
      "Generated files will be available in folder /Users/smukherjee/Documents/CODE/Github/OpenML-auto-automl/automlbenchmark/results.\n",
      "Running cmd `docker run --name randomforest.openml_t_49.test.docker.20241009T153843.sIiURqJG8Z99apxsQC8ISg__ --shm-size=2048M  -v /Users/smukherjee/.openml:/input -v /Users/smukherjee/Documents/CODE/Github/OpenML-auto-automl/automlbenchmark/results/randomforest.openml_t_49.test.docker.20241009T153843:/output -v /Users/smukherjee/.config/automlbenchmark:/custom --rm automlbenchmark/randomforest:stable randomforest openml/t/49 test   -Xseed=auto -i /input -o /output -u /custom -s skip -Xrun_mode=docker --session=`\n",
      "WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n",
      "Running benchmark `randomforest` on `openml/t/49` framework in `local` mode.\n",
      "Loading frameworks definitions from ['/bench/resources/frameworks.yaml'].\n",
      "Loading benchmark constraint definitions from ['/bench/resources/constraints.yaml'].\n",
      "Loading openml task 49.\n",
      "[MONITORING] [python3.7 [1]] CPU Utilization: 18.4%\n",
      "[MONITORING] [python3.7 [1]] Memory Usage: 5.0%\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Starting job local.openml_t_49.test.tic-tac-toe.0.RandomForest.\n",
      "[MONITORING] [python3.7 [1]] Disk Usage: 76.1%\n",
      "Assigning 4 cores (total=11) for new task tic-tac-toe.\n",
      "Assigning 10744 MB (total=13462 MB) for new tic-tac-toe task.\n",
      "Running task tic-tac-toe on framework RandomForest with config:\n",
      "TaskConfig({})\n",
      "Running cmd `/bench/frameworks/RandomForest/venv/bin/python -W ignore /bench/frameworks/RandomForest/exec.py`\n",
      "INFO:exec.py:\n",
      "**** Random Forest [sklearn v1.0] ****\n",
      "\n",
      "\n",
      "\n",
      "INFO:exec.py:Running RandomForest with a maximum time of 600s on 4 cores.\n",
      "\n",
      "WARNING:exec.py:We completely ignore the requirement to stay within the time limit.\n",
      "\n",
      "WARNING:exec.py:We completely ignore the advice to optimize towards metric: auc.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predictions preview:\n",
      "     negative  positive predictions     truth\n",
      "0     0.1260    0.8740    positive  positive\n",
      "1     0.2070    0.7930    positive  positive\n",
      "2     0.1700    0.8300    positive  positive\n",
      "3     0.1685    0.8315    positive  positive\n",
      "4     0.1560    0.8440    positive  positive\n",
      "5     0.0540    0.9460    positive  positive\n",
      "6     0.0550    0.9450    positive  positive\n",
      "7     0.0750    0.9250    positive  positive\n",
      "8     0.0875    0.9125    positive  positive\n",
      "9     0.0280    0.9720    positive  positive\n",
      "10    0.0795    0.9205    positive  positive\n",
      "11    0.0495    0.9505    positive  positive\n",
      "12    0.3390    0.6610    positive  positive\n",
      "13    0.1830    0.8170    positive  positive\n",
      "14    0.0735    0.9265    positive  positive\n",
      "15    0.1705    0.8295    positive  positive\n",
      "16    0.0370    0.9630    positive  positive\n",
      "17    0.1180    0.8820    positive  positive\n",
      "18    0.1245    0.8755    positive  positive\n",
      "19    0.1870    0.8130    positive  positive\n",
      "\n",
      "Predictions saved to `/output/predictions/tic-tac-toe/0/predictions.csv`.\n",
      "Loading metadata from `/output/predictions/tic-tac-toe/0/metadata.json`.\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "\n",
      "Loading predictions from `/output/predictions/tic-tac-toe/0/predictions.csv`.\n",
      "Metric scores: { 'acc': 0.96875,\n",
      "  'app_version': 'dev [NA, NA, NA]',\n",
      "  'auc': 1.0,\n",
      "  'balacc': 0.9545454545454546,\n",
      "  'constraint': 'test',\n",
      "  'duration': nan,\n",
      "  'fold': 0,\n",
      "  'framework': 'RandomForest',\n",
      "  'id': 'openml.org/t/49',\n",
      "  'info': None,\n",
      "  'logloss': 0.21935486367102217,\n",
      "  'metric': 'auc',\n",
      "  'mode': 'docker',\n",
      "  'models_count': 2000,\n",
      "  'params': \"{'n_estimators': 2000}\",\n",
      "  'predict_duration': 1.0324931144714355,\n",
      "  'result': 1.0,\n",
      "  'seed': 1475303368,\n",
      "  'task': 'tic-tac-toe',\n",
      "  'training_duration': 11.24776291847229,\n",
      "  'type': 'binary',\n",
      "  'utc': '2024-10-09T15:39:06',\n",
      "  'version': '1.0'}\n",
      "Job `local.openml_t_49.test.tic-tac-toe.0.RandomForest` executed in 16.887 seconds.\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Starting job local.openml_t_49.test.tic-tac-toe.1.RandomForest.\n",
      "Assigning 4 cores (total=11) for new task tic-tac-toe.\n",
      "Assigning 10737 MB (total=13462 MB) for new tic-tac-toe task.\n",
      "Running task tic-tac-toe on framework RandomForest with config:\n",
      "TaskConfig({})\n",
      "Running cmd `/bench/frameworks/RandomForest/venv/bin/python -W ignore /bench/frameworks/RandomForest/exec.py`\n",
      "INFO:exec.py:\n",
      "**** Random Forest [sklearn v1.0] ****\n",
      "\n",
      "INFO:exec.py:Running RandomForest with a maximum time of 600s on 4 cores.\n",
      "WARNING:exec.py:We completely ignore the requirement to stay within the time limit.\n",
      "WARNING:exec.py:We completely ignore the advice to optimize towards metric: auc.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predictions preview:\n",
      "     negative  positive predictions     truth\n",
      "0     0.0900    0.9100    positive  positive\n",
      "1     0.0370    0.9630    positive  positive\n",
      "2     0.0415    0.9585    positive  positive\n",
      "3     0.2045    0.7955    positive  positive\n",
      "4     0.1070    0.8930    positive  positive\n",
      "5     0.2105    0.7895    positive  positive\n",
      "6     0.0835    0.9165    positive  positive\n",
      "7     0.0250    0.9750    positive  positive\n",
      "8     0.0400    0.9600    positive  positive\n",
      "9     0.1900    0.8100    positive  positive\n",
      "10    0.2285    0.7715    positive  positive\n",
      "11    0.3385    0.6615    positive  positive\n",
      "12    0.1625    0.8375    positive  positive\n",
      "13    0.0335    0.9665    positive  positive\n",
      "14    0.0835    0.9165    positive  positive\n",
      "15    0.1415    0.8585    positive  positive\n",
      "16    0.0455    0.9545    positive  positive\n",
      "17    0.0470    0.9530    positive  positive\n",
      "18    0.0235    0.9765    positive  positive\n",
      "19    0.1290    0.8710    positive  positive\n",
      "\n",
      "Predictions saved to `/output/predictions/tic-tac-toe/1/predictions.csv`.\n",
      "Loading metadata from `/output/predictions/tic-tac-toe/1/metadata.json`.\n",
      "Loading predictions from `/output/predictions/tic-tac-toe/1/predictions.csv`.\n",
      "Metric scores: { 'acc': 0.96875,\n",
      "  'app_version': 'dev [NA, NA, NA]',\n",
      "  'auc': 1.0,\n",
      "  'balacc': 0.9545454545454546,\n",
      "  'constraint': 'test',\n",
      "  'duration': nan,\n",
      "  'fold': 1,\n",
      "  'framework': 'RandomForest',\n",
      "  'id': 'openml.org/t/49',\n",
      "  'info': None,\n",
      "  'logloss': 0.20959471547170616,\n",
      "  'metric': 'auc',\n",
      "  'mode': 'docker',\n",
      "  'models_count': 2000,\n",
      "  'params': \"{'n_estimators': 2000}\",\n",
      "  'predict_duration': 1.0233509540557861,\n",
      "  'result': 1.0,\n",
      "  'seed': 1475303369,\n",
      "  'task': 'tic-tac-toe',\n",
      "  'training_duration': 11.208200693130493,\n",
      "  'type': 'binary',\n",
      "  'utc': '2024-10-09T15:39:23',\n",
      "  'version': '1.0'}\n",
      "Job `local.openml_t_49.test.tic-tac-toe.1.RandomForest` executed in 16.766 seconds.\n",
      "All jobs executed in 33.697 seconds.\n",
      "[MONITORING] [python3.7 [1]] CPU Utilization: 10.6%\n",
      "[MONITORING] [python3.7 [1]] Memory Usage: 5.2%\n",
      "[MONITORING] [python3.7 [1]] Disk Usage: 76.1%\n",
      "Processing results for \n",
      "Scores saved to `/output/scores/RandomForest.benchmark_openml_t_49.csv`.\n",
      "Scores saved to `/output/scores/results.csv`.\n",
      "Scores saved to `/output/results.csv`.\n",
      "Summing up scores for current run:\n",
      "             id        task fold    framework constraint  result metric  duration       seed\n",
      "openml.org/t/49 tic-tac-toe    0 RandomForest       test     1.0    auc      16.9 1475303368\n",
      "openml.org/t/49 tic-tac-toe    1 RandomForest       test     1.0    auc      16.8 1475303369\n",
      "Job `docker.openml_t_49.test.all_tasks.all_folds.RandomForest` executed in 38.700 seconds.\n",
      "All jobs executed in 38.702 seconds.\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] CPU Utilization: 28.1%\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] Memory Usage: 62.4%\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] Disk Usage: 6.5%\n",
      "Summing up scores for current run:\n",
      "             id        task  fold    framework constraint  result metric  duration       seed\n",
      "openml.org/t/49 tic-tac-toe     0 RandomForest       test     1.0    auc      16.9 1475303368\n",
      "openml.org/t/49 tic-tac-toe     1 RandomForest       test     1.0    auc      16.8 1475303369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing results for task 49: 100%|██████████| 2/2 [00:00<00:00, 18157.16it/s]\n",
      "Running benchmark on dataset 50: 100%|██████████| 1/1 [00:41<00:00, 41.49s/it]\n"
     ]
    }
   ],
   "source": [
    "tf.run_benchmark_on_all_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 2\u001b[0m store \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mglobal_results_store\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m datasets \u001b[38;5;129;01min\u001b[39;00m store\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      4\u001b[0m     result_file \u001b[38;5;241m=\u001b[39m Path(store[datasets][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_config\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores/results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "store = tf.global_results_store\n",
    "for datasets in store.keys():\n",
    "    result_file = Path(store[datasets][\"task_config\"][\"output_dir\"])/\"scores/results.csv\"\n",
    "    pandas_file = pd.read_csv(result_file)\n",
    "    print(pandas_file.head())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'task', 'framework', 'constraint', 'fold', 'type', 'result',\n",
       "       'metric', 'mode', 'version', 'params', 'app_version', 'utc', 'duration',\n",
       "       'training_duration', 'predict_duration', 'models_count', 'seed', 'info',\n",
       "       'acc', 'auc', 'balacc', 'logloss'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_file.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'framework': 'RandomForest', 'framework_params': {'n_estimators': 2000}, 'framework_version': '1.0', 'type': 'classification', 'name': 'anneal', 'fold': 1, 'metric': 'logloss', 'metrics': ['logloss', 'acc', 'balacc'], 'seed': 1095784781, 'job_timeout_seconds': 1200, 'max_runtime_seconds': 600, 'cores': 4, 'max_mem_size_mb': 10721, 'min_vol_size_mb': -1, 'input_dir': '/input', 'output_dir': '/output/', 'output_predictions_file': '/output/predictions/anneal/1/predictions.csv', 'ext': {}, 'type_': 'multiclass', 'output_metadata_file': '/output/predictions/anneal/1/metadata.json'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x330402d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Dash app\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "# Layout\n",
    "app.layout = dbc.Container([\n",
    "    dbc.Row([\n",
    "        dbc.Col(html.H1(\"Model Results Dashboard\"), width=12)\n",
    "    ]),\n",
    "    \n",
    "    dbc.Row([\n",
    "        dbc.Col([\n",
    "            html.Label(\"Select Metric:\"),\n",
    "            dcc.Dropdown(\n",
    "                id='metric-dropdown',\n",
    "                options=[\n",
    "                    {'label': 'Accuracy', 'value': 'acc'},\n",
    "                    {'label': 'AUC', 'value': 'auc'},\n",
    "                    {'label': 'Log Loss', 'value': 'logloss'}\n",
    "                ],\n",
    "                value='acc',\n",
    "                clearable=False\n",
    "            )\n",
    "        ], width=4),\n",
    "    ]),\n",
    "\n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Graph(id='metric-graph'), width=12)\n",
    "    ]),\n",
    "\n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Graph(id='task-pie-chart'), width=6),\n",
    "        dbc.Col(dcc.Graph(id='framework-bar-chart'), width=6),\n",
    "    ]),\n",
    "\n",
    "], fluid=True)\n",
    "\n",
    "# Callbacks\n",
    "@app.callback(\n",
    "    Output('metric-graph', 'figure'),\n",
    "    Output('task-pie-chart', 'figure'),\n",
    "    Output('framework-bar-chart', 'figure'),\n",
    "    Input('metric-dropdown', 'value')\n",
    ")\n",
    "\n",
    "\n",
    "# Run app and display result inline in the notebook\n",
    "app.run_server(mode='inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_task_id_from_folder_name(folder_name):\n",
    "    name = re.findall(pattern=r\"_t_.*[0-9]\\.\", string=folder_name)\n",
    "    print(name)\n",
    "    if isinstance(name, list) and len(name) > 0:\n",
    "        return str(name[0])\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_t_5.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'_t_5.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_task_id_from_folder_name(folder_name=\"randomforest.openml_t_5.test.docker.20241016T141320\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = openml.tasks.get_task(10, download_data=False, download_qualities= False)\n",
    "t.dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automlb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
