{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "openml.config.apikey = openml.config.get_config_as_dict()['apikey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = openml.datasets.get_dataset(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import CategoricalDtype\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_target_col_type(dataset, target_col_name):\n",
    "    try:\n",
    "        if dataset.features:\n",
    "            return next((feature.data_type for feature in dataset.features.values() if feature.name == target_col_name), None)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "def check_if_api_key_is_valid():\n",
    "    if not openml.config.get_config_as_dict()['apikey']:\n",
    "        print(\"API key is not set. Please set the API key using openml.config.apikey = 'your-key'\")\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def try_create_task(dataset_id):\n",
    "    try:\n",
    "        dataset = openml.datasets.get_dataset(dataset_id)\n",
    "        target_col_name = dataset.default_target_attribute\n",
    "        target_col_type = get_target_col_type(dataset, target_col_name)\n",
    "\n",
    "        if target_col_type:\n",
    "            if target_col_type in ['nominal', 'string', 'categorical']:\n",
    "                evaluation_measure=\"predictive_accuracy\"\n",
    "                task_type = openml.tasks.TaskType.SUPERVISED_CLASSIFICATION\n",
    "            elif target_col_type == 'numeric':\n",
    "                evaluation_measure=\"mean_absolute_error\"\n",
    "                task_type = openml.tasks.TaskType.SUPERVISED_REGRESSION\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "            task = openml.tasks.create_task(\n",
    "                dataset_id=dataset_id,\n",
    "                task_type=task_type,\n",
    "                target_name=target_col_name,\n",
    "                evaluation_measure=evaluation_measure,\n",
    "                estimation_procedure_id=1)\n",
    "            # try:\n",
    "            if check_if_api_key_is_valid():\n",
    "                task.publish()\n",
    "            else:\n",
    "                return None\n",
    "            print(f\"Task created: {task}, task_id: {task.task_id}\")\n",
    "            return task.task_id\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task created: OpenML Classification Task\n",
      "==========================\n",
      "Task Type Description: https://www.openml.org/tt/TaskType.SUPERVISED_CLASSIFICATION\n",
      "Task ID..............: 362130\n",
      "Task URL.............: https://www.openml.org/t/362130\n",
      "Estimation Procedure.: None\n",
      "Evaluation Measure...: predictive_accuracy\n",
      "Target Feature.......: result\n",
      "Cost Matrix..........: Available, task_id: 362130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "362130"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_create_task(46342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- todo\n",
    "  - requirements to poetry\n",
    "  - install all required frameworks\n",
    "  - check if results exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database and delete rows that have framework==autogluon\n",
    "conn = sqlite3.connect('./data/runs.db')\n",
    "c = conn.cursor()\n",
    "c.execute(\"DELETE FROM runs WHERE framework='gama'\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 2, 'randomforest'), (3, 3, 'randomforest'), (4, 4, 'randomforest'), (5, 5, 'randomforest'), (6, 6, 'randomforest'), (7, 7, 'randomforest'), (8, 8, 'randomforest'), (9, 9, 'randomforest'), (10, 10, 'randomforest'), (11, 11, 'randomforest')]\n"
     ]
    }
   ],
   "source": [
    "# select * from runs\n",
    "c.execute(\"SELECT * FROM runs\")\n",
    "print(c.fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenML Dataset\n",
       "==============\n",
       "Name..........: anneal\n",
       "Version.......: 2\n",
       "Format........: ARFF\n",
       "Upload Date...: 2014-04-06 23:19:20\n",
       "Licence.......: Public\n",
       "Download URL..: https://api.openml.org/data/v1/download/1/anneal.arff\n",
       "OpenML URL....: https://www.openml.org/d/1\n",
       "# of features.: 39\n",
       "# of instances: 898"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openml.datasets.get_dataset(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smukherjee/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm.auto import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskConfig:\n",
    "    def __init__(self, *args):\n",
    "        self.args = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskFinder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        testing_mode=True,\n",
    "        use_cache=True,\n",
    "        run_mode=\"docker\",\n",
    "        num_tasks_to_return=1,\n",
    "        save_every_n_tasks=10,\n",
    "    ):\n",
    "        self.testing_mode = testing_mode\n",
    "        self.cache_file_name = \"data/dataset_list.csv\"\n",
    "        self.global_results_store = {}\n",
    "        self.num_tasks_to_return = num_tasks_to_return\n",
    "        self.use_cache = use_cache\n",
    "        self.save_every_n_tasks = save_every_n_tasks\n",
    "        # self.benchmarks_to_use = [\"autosklearn\"]\n",
    "        self.benchmarks_to_use = [\"randomforest\", \"autogluon\"]\n",
    "        # Ensure required folders exist\n",
    "        self.make_files([\"data\"])\n",
    "        self.run_mode = run_mode\n",
    "\n",
    "        self.check_run_mode()\n",
    "        # Load datasets from cache or OpenML\n",
    "        self.datasets = self.load_datasets()\n",
    "\n",
    "        # If in testing mode, only take the first dataset\n",
    "        if self.testing_mode:\n",
    "            # self.datasets = self.datasets.head(1)\n",
    "            # 43972 - tic tac\n",
    "            self.datasets = self.datasets[self.datasets[\"did\"] == 50]\n",
    "\n",
    "    def check_run_mode(self):\n",
    "        possible = [\"local\", \"aws\", \"docker\", \"singularity\"]\n",
    "        if self.run_mode not in possible:\n",
    "            raise ValueError(\n",
    "                f\"Invalid run mode: {self.run_mode}. Possible values are: {possible}\"\n",
    "            )\n",
    "\n",
    "    def load_datasets(self):\n",
    "        datasets = openml.datasets.list_datasets(output_format=\"dataframe\")\n",
    "        # if the cache file exists, append the new unique datasets to it\n",
    "        if self.use_cache and os.path.exists(self.cache_file_name):\n",
    "            cached_datasets = pd.read_csv(self.cache_file_name)\n",
    "        else:\n",
    "            if os.path.exists(self.cache_file_name):\n",
    "                cached_datasets = pd.read_csv(self.cache_file_name)\n",
    "                datasets = pd.concat([datasets, cached_datasets], ignore_index=True)\n",
    "                datasets = datasets.drop_duplicates(subset=[\"did\"])\n",
    "\n",
    "                # Save the updated dataset list to the cache\n",
    "                datasets.to_csv(self.cache_file_name)\n",
    "        return datasets\n",
    "\n",
    "    def make_files(self, folders):\n",
    "        for folder in folders:\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    def get_tasks_from_dataset(self, dataset_id, num_tasks_to_return=1):\n",
    "        try:\n",
    "            tasks = openml.tasks.list_tasks(\n",
    "                data_id=dataset_id, output_format=\"dataframe\"\n",
    "            )\n",
    "            # if the task column estimation_procedure is not 10-fold Crossvalidation drop the row\n",
    "            tasks = tasks[tasks[\"estimation_procedure\"] == \"10-fold Crossvalidation\"]\n",
    "            # return the first num_tasks_to_return tasks\n",
    "            tasks = tasks.head(num_tasks_to_return)\n",
    "            return tasks[\"tid\"].tolist() if not len(tasks) == 0 else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving tasks for dataset {dataset_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def run_all_benchmarks_on_task(self, task_id):\n",
    "        # task_id = task_id.strip()\n",
    "        for benchmark_type in self.benchmarks_to_use:\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\n",
    "                        \"python3\",\n",
    "                        \"automlbenchmark/runbenchmark.py\",\n",
    "                        benchmark_type,\n",
    "                        f\"openml/t/{task_id}\",\n",
    "                        \"--mode\",\n",
    "                        self.run_mode,\n",
    "                    ],\n",
    "                    text=True,\n",
    "                    capture_output=True,\n",
    "                )\n",
    "                print(result.stderr)\n",
    "                # Filter and return relevant benchmark output\n",
    "                if self.run_mode == \"local\":\n",
    "                    return [\n",
    "                        line for line in result.stderr.split(\"\\n\") if \"TaskConfig\" in line\n",
    "                    ]\n",
    "                elif self.run_mode == \"docker\":\n",
    "                   return [\n",
    "                        line for line in result.stderr.split(\"\\n\") if \"Starting docker: docker run --name \" in line\n",
    "                    ] \n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Benchmark run failed for task {task_id}: {e}\")\n",
    "                return []\n",
    "\n",
    "    def get_task_config_from_str(self, task_str):\n",
    "        try:\n",
    "            if self.run_mode == \"local\":\n",
    "                return eval(task_str).__dict__[\"args\"][0]\n",
    "            elif self.run_mode == \"docker\":\n",
    "                # --name randomforest.openml_t_49.test.docker.20241009T153843.sIiURqJG8Z99apxsQC8ISg__ \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing task config: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_task_for_dataset(self, dataset_id):\n",
    "        task_ids = self.get_tasks_from_dataset(dataset_id, self.num_tasks_to_return)\n",
    "        if task_ids:\n",
    "            for task_id in tqdm(\n",
    "                task_ids, desc=f\"Running benchmark on dataset {dataset_id}\"\n",
    "            ):\n",
    "                benchmark_results = self.run_all_benchmarks_on_task(\n",
    "                    task_id,\n",
    "                )\n",
    "                for result in tqdm(\n",
    "                    benchmark_results, desc=f\"Processing results for task {task_id}\"\n",
    "                ):\n",
    "                    task_config = self.get_task_config_from_str(result)\n",
    "                    if task_config and self.run_mode == \"local\":\n",
    "                        current_run_info = {\n",
    "                            \"dataset_id\": dataset_id,\n",
    "                            \"task_id\": task_id,\n",
    "                            \"task_config\": task_config,\n",
    "                        }\n",
    "                        self.global_results_store[dataset_id] = current_run_info\n",
    "                    if task_config and self.run_mode == \"docker\":\n",
    "                        current_run_info = {\n",
    "                            \"dataset_id\": dataset_id,\n",
    "                            \"task_id\": task_id,\n",
    "                        }\n",
    "                # write the results to a file every 10 tasks\n",
    "                if len(self.global_results_store) % self.save_every_n_tasks == 0:\n",
    "                    self.write_global_to_file()\n",
    "\n",
    "    def write_global_to_file(\n",
    "        self, file_name=\"data/links_to_automl_files_per_dataset.json\"\n",
    "    ):\n",
    "        # read the current file if it exists and append the new results\n",
    "        if os.path.exists(file_name):\n",
    "            with open(file_name, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                data.update(self.global_results_store)\n",
    "        else:\n",
    "            data = self.global_results_store\n",
    "        with open(file_name, \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "    def load_global_from_file(\n",
    "        self, file_name=\"data/links_to_automl_files_per_dataset.json\"\n",
    "    ):\n",
    "        if os.path.exists(file_name):\n",
    "            with open(file_name, \"r\") as f:\n",
    "                self.global_results_store = json.load(f)\n",
    "        else:\n",
    "            print(\"No global results file found\")\n",
    "\n",
    "    def run_benchmark_on_all_datasets(self):\n",
    "        for _, row in self.datasets.iterrows():\n",
    "            dataset_id = row[\"did\"]\n",
    "            self.get_task_for_dataset(dataset_id)\n",
    "\n",
    "        # save info\n",
    "        self.write_global_to_file()\n",
    "\n",
    "    def upload_results_to_openml(self):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TaskFinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running benchmark on dataset 50:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/smukherjee/.pyenv/versions/3.9.19/envs/automlb/lib/python3.9/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "Running benchmark `randomforest` on `openml/t/49` framework in `docker` mode.\n",
      "Loading frameworks definitions from ['/Users/smukherjee/Documents/CODE/Github/OpenML-auto-automl/automlbenchmark/resources/frameworks.yaml'].\n",
      "Loading benchmark constraint definitions from ['/Users/smukherjee/Documents/CODE/Github/OpenML-auto-automl/automlbenchmark/resources/constraints.yaml'].\n",
      "Loading openml task 49.\n",
      "Running cmd `docker images -q automlbenchmark/randomforest:stable-dev`\n",
      "Running cmd `docker pull automlbenchmark/randomforest:stable-dev`\n",
      "Error response from daemon: manifest for automlbenchmark/randomforest:stable-dev not found: manifest unknown: manifest unknown\n",
      "\n",
      "\n",
      "\n",
      "Error response from daemon: manifest for automlbenchmark/randomforest:stable-dev not found: manifest unknown: manifest unknown\n",
      "\n",
      "Running cmd `docker images -q automlbenchmark/randomforest:stable`\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] CPU Utilization: 26.1%\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Starting job docker.openml_t_49.test.all_tasks.all_folds.RandomForest.\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] Memory Usage: 63.4%\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] Disk Usage: 6.5%\n",
      "Starting docker: docker run --name randomforest.openml_t_49.test.docker.20241009T153843.sIiURqJG8Z99apxsQC8ISg__ --shm-size=2048M  -v /Users/smukherjee/.openml:/input -v /Users/smukherjee/Documents/CODE/Github/OpenML-auto-automl/automlbenchmark/results/randomforest.openml_t_49.test.docker.20241009T153843:/output -v /Users/smukherjee/.config/automlbenchmark:/custom --rm automlbenchmark/randomforest:stable randomforest openml/t/49 test   -Xseed=auto -i /input -o /output -u /custom -s skip -Xrun_mode=docker --session=.\n",
      "Datasets are loaded by default from folder /Users/smukherjee/.openml.\n",
      "Generated files will be available in folder /Users/smukherjee/Documents/CODE/Github/OpenML-auto-automl/automlbenchmark/results.\n",
      "Running cmd `docker run --name randomforest.openml_t_49.test.docker.20241009T153843.sIiURqJG8Z99apxsQC8ISg__ --shm-size=2048M  -v /Users/smukherjee/.openml:/input -v /Users/smukherjee/Documents/CODE/Github/OpenML-auto-automl/automlbenchmark/results/randomforest.openml_t_49.test.docker.20241009T153843:/output -v /Users/smukherjee/.config/automlbenchmark:/custom --rm automlbenchmark/randomforest:stable randomforest openml/t/49 test   -Xseed=auto -i /input -o /output -u /custom -s skip -Xrun_mode=docker --session=`\n",
      "WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n",
      "Running benchmark `randomforest` on `openml/t/49` framework in `local` mode.\n",
      "Loading frameworks definitions from ['/bench/resources/frameworks.yaml'].\n",
      "Loading benchmark constraint definitions from ['/bench/resources/constraints.yaml'].\n",
      "Loading openml task 49.\n",
      "[MONITORING] [python3.7 [1]] CPU Utilization: 18.4%\n",
      "[MONITORING] [python3.7 [1]] Memory Usage: 5.0%\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Starting job local.openml_t_49.test.tic-tac-toe.0.RandomForest.\n",
      "[MONITORING] [python3.7 [1]] Disk Usage: 76.1%\n",
      "Assigning 4 cores (total=11) for new task tic-tac-toe.\n",
      "Assigning 10744 MB (total=13462 MB) for new tic-tac-toe task.\n",
      "Running task tic-tac-toe on framework RandomForest with config:\n",
      "TaskConfig({})\n",
      "Running cmd `/bench/frameworks/RandomForest/venv/bin/python -W ignore /bench/frameworks/RandomForest/exec.py`\n",
      "INFO:exec.py:\n",
      "**** Random Forest [sklearn v1.0] ****\n",
      "\n",
      "\n",
      "\n",
      "INFO:exec.py:Running RandomForest with a maximum time of 600s on 4 cores.\n",
      "\n",
      "WARNING:exec.py:We completely ignore the requirement to stay within the time limit.\n",
      "\n",
      "WARNING:exec.py:We completely ignore the advice to optimize towards metric: auc.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predictions preview:\n",
      "     negative  positive predictions     truth\n",
      "0     0.1260    0.8740    positive  positive\n",
      "1     0.2070    0.7930    positive  positive\n",
      "2     0.1700    0.8300    positive  positive\n",
      "3     0.1685    0.8315    positive  positive\n",
      "4     0.1560    0.8440    positive  positive\n",
      "5     0.0540    0.9460    positive  positive\n",
      "6     0.0550    0.9450    positive  positive\n",
      "7     0.0750    0.9250    positive  positive\n",
      "8     0.0875    0.9125    positive  positive\n",
      "9     0.0280    0.9720    positive  positive\n",
      "10    0.0795    0.9205    positive  positive\n",
      "11    0.0495    0.9505    positive  positive\n",
      "12    0.3390    0.6610    positive  positive\n",
      "13    0.1830    0.8170    positive  positive\n",
      "14    0.0735    0.9265    positive  positive\n",
      "15    0.1705    0.8295    positive  positive\n",
      "16    0.0370    0.9630    positive  positive\n",
      "17    0.1180    0.8820    positive  positive\n",
      "18    0.1245    0.8755    positive  positive\n",
      "19    0.1870    0.8130    positive  positive\n",
      "\n",
      "Predictions saved to `/output/predictions/tic-tac-toe/0/predictions.csv`.\n",
      "Loading metadata from `/output/predictions/tic-tac-toe/0/metadata.json`.\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "\n",
      "Loading predictions from `/output/predictions/tic-tac-toe/0/predictions.csv`.\n",
      "Metric scores: { 'acc': 0.96875,\n",
      "  'app_version': 'dev [NA, NA, NA]',\n",
      "  'auc': 1.0,\n",
      "  'balacc': 0.9545454545454546,\n",
      "  'constraint': 'test',\n",
      "  'duration': nan,\n",
      "  'fold': 0,\n",
      "  'framework': 'RandomForest',\n",
      "  'id': 'openml.org/t/49',\n",
      "  'info': None,\n",
      "  'logloss': 0.21935486367102217,\n",
      "  'metric': 'auc',\n",
      "  'mode': 'docker',\n",
      "  'models_count': 2000,\n",
      "  'params': \"{'n_estimators': 2000}\",\n",
      "  'predict_duration': 1.0324931144714355,\n",
      "  'result': 1.0,\n",
      "  'seed': 1475303368,\n",
      "  'task': 'tic-tac-toe',\n",
      "  'training_duration': 11.24776291847229,\n",
      "  'type': 'binary',\n",
      "  'utc': '2024-10-09T15:39:06',\n",
      "  'version': '1.0'}\n",
      "Job `local.openml_t_49.test.tic-tac-toe.0.RandomForest` executed in 16.887 seconds.\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Starting job local.openml_t_49.test.tic-tac-toe.1.RandomForest.\n",
      "Assigning 4 cores (total=11) for new task tic-tac-toe.\n",
      "Assigning 10737 MB (total=13462 MB) for new tic-tac-toe task.\n",
      "Running task tic-tac-toe on framework RandomForest with config:\n",
      "TaskConfig({})\n",
      "Running cmd `/bench/frameworks/RandomForest/venv/bin/python -W ignore /bench/frameworks/RandomForest/exec.py`\n",
      "INFO:exec.py:\n",
      "**** Random Forest [sklearn v1.0] ****\n",
      "\n",
      "INFO:exec.py:Running RandomForest with a maximum time of 600s on 4 cores.\n",
      "WARNING:exec.py:We completely ignore the requirement to stay within the time limit.\n",
      "WARNING:exec.py:We completely ignore the advice to optimize towards metric: auc.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predictions preview:\n",
      "     negative  positive predictions     truth\n",
      "0     0.0900    0.9100    positive  positive\n",
      "1     0.0370    0.9630    positive  positive\n",
      "2     0.0415    0.9585    positive  positive\n",
      "3     0.2045    0.7955    positive  positive\n",
      "4     0.1070    0.8930    positive  positive\n",
      "5     0.2105    0.7895    positive  positive\n",
      "6     0.0835    0.9165    positive  positive\n",
      "7     0.0250    0.9750    positive  positive\n",
      "8     0.0400    0.9600    positive  positive\n",
      "9     0.1900    0.8100    positive  positive\n",
      "10    0.2285    0.7715    positive  positive\n",
      "11    0.3385    0.6615    positive  positive\n",
      "12    0.1625    0.8375    positive  positive\n",
      "13    0.0335    0.9665    positive  positive\n",
      "14    0.0835    0.9165    positive  positive\n",
      "15    0.1415    0.8585    positive  positive\n",
      "16    0.0455    0.9545    positive  positive\n",
      "17    0.0470    0.9530    positive  positive\n",
      "18    0.0235    0.9765    positive  positive\n",
      "19    0.1290    0.8710    positive  positive\n",
      "\n",
      "Predictions saved to `/output/predictions/tic-tac-toe/1/predictions.csv`.\n",
      "Loading metadata from `/output/predictions/tic-tac-toe/1/metadata.json`.\n",
      "Loading predictions from `/output/predictions/tic-tac-toe/1/predictions.csv`.\n",
      "Metric scores: { 'acc': 0.96875,\n",
      "  'app_version': 'dev [NA, NA, NA]',\n",
      "  'auc': 1.0,\n",
      "  'balacc': 0.9545454545454546,\n",
      "  'constraint': 'test',\n",
      "  'duration': nan,\n",
      "  'fold': 1,\n",
      "  'framework': 'RandomForest',\n",
      "  'id': 'openml.org/t/49',\n",
      "  'info': None,\n",
      "  'logloss': 0.20959471547170616,\n",
      "  'metric': 'auc',\n",
      "  'mode': 'docker',\n",
      "  'models_count': 2000,\n",
      "  'params': \"{'n_estimators': 2000}\",\n",
      "  'predict_duration': 1.0233509540557861,\n",
      "  'result': 1.0,\n",
      "  'seed': 1475303369,\n",
      "  'task': 'tic-tac-toe',\n",
      "  'training_duration': 11.208200693130493,\n",
      "  'type': 'binary',\n",
      "  'utc': '2024-10-09T15:39:23',\n",
      "  'version': '1.0'}\n",
      "Job `local.openml_t_49.test.tic-tac-toe.1.RandomForest` executed in 16.766 seconds.\n",
      "All jobs executed in 33.697 seconds.\n",
      "[MONITORING] [python3.7 [1]] CPU Utilization: 10.6%\n",
      "[MONITORING] [python3.7 [1]] Memory Usage: 5.2%\n",
      "[MONITORING] [python3.7 [1]] Disk Usage: 76.1%\n",
      "Processing results for \n",
      "Scores saved to `/output/scores/RandomForest.benchmark_openml_t_49.csv`.\n",
      "Scores saved to `/output/scores/results.csv`.\n",
      "Scores saved to `/output/results.csv`.\n",
      "Summing up scores for current run:\n",
      "             id        task fold    framework constraint  result metric  duration       seed\n",
      "openml.org/t/49 tic-tac-toe    0 RandomForest       test     1.0    auc      16.9 1475303368\n",
      "openml.org/t/49 tic-tac-toe    1 RandomForest       test     1.0    auc      16.8 1475303369\n",
      "Job `docker.openml_t_49.test.all_tasks.all_folds.RandomForest` executed in 38.700 seconds.\n",
      "All jobs executed in 38.702 seconds.\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] CPU Utilization: 28.1%\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] Memory Usage: 62.4%\n",
      "[MONITORING] [docker.openml_t_49.test.all_tasks.all_folds.RandomForest] Disk Usage: 6.5%\n",
      "Summing up scores for current run:\n",
      "             id        task  fold    framework constraint  result metric  duration       seed\n",
      "openml.org/t/49 tic-tac-toe     0 RandomForest       test     1.0    auc      16.9 1475303368\n",
      "openml.org/t/49 tic-tac-toe     1 RandomForest       test     1.0    auc      16.8 1475303369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing results for task 49: 100%|██████████| 2/2 [00:00<00:00, 18157.16it/s]\n",
      "Running benchmark on dataset 50: 100%|██████████| 1/1 [00:41<00:00, 41.49s/it]\n"
     ]
    }
   ],
   "source": [
    "tf.run_benchmark_on_all_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 2\u001b[0m store \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mglobal_results_store\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m datasets \u001b[38;5;129;01min\u001b[39;00m store\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      4\u001b[0m     result_file \u001b[38;5;241m=\u001b[39m Path(store[datasets][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_config\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores/results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "store = tf.global_results_store\n",
    "for datasets in store.keys():\n",
    "    result_file = Path(store[datasets][\"task_config\"][\"output_dir\"])/\"scores/results.csv\"\n",
    "    pandas_file = pd.read_csv(result_file)\n",
    "    print(pandas_file.head())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'task', 'framework', 'constraint', 'fold', 'type', 'result',\n",
       "       'metric', 'mode', 'version', 'params', 'app_version', 'utc', 'duration',\n",
       "       'training_duration', 'predict_duration', 'models_count', 'seed', 'info',\n",
       "       'acc', 'auc', 'balacc', 'logloss'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_file.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'framework': 'RandomForest', 'framework_params': {'n_estimators': 2000}, 'framework_version': '1.0', 'type': 'classification', 'name': 'anneal', 'fold': 1, 'metric': 'logloss', 'metrics': ['logloss', 'acc', 'balacc'], 'seed': 1095784781, 'job_timeout_seconds': 1200, 'max_runtime_seconds': 600, 'cores': 4, 'max_mem_size_mb': 10721, 'min_vol_size_mb': -1, 'input_dir': '/input', 'output_dir': '/output/', 'output_predictions_file': '/output/predictions/anneal/1/predictions.csv', 'ext': {}, 'type_': 'multiclass', 'output_metadata_file': '/output/predictions/anneal/1/metadata.json'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x330402d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Dash app\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "# Layout\n",
    "app.layout = dbc.Container([\n",
    "    dbc.Row([\n",
    "        dbc.Col(html.H1(\"Model Results Dashboard\"), width=12)\n",
    "    ]),\n",
    "    \n",
    "    dbc.Row([\n",
    "        dbc.Col([\n",
    "            html.Label(\"Select Metric:\"),\n",
    "            dcc.Dropdown(\n",
    "                id='metric-dropdown',\n",
    "                options=[\n",
    "                    {'label': 'Accuracy', 'value': 'acc'},\n",
    "                    {'label': 'AUC', 'value': 'auc'},\n",
    "                    {'label': 'Log Loss', 'value': 'logloss'}\n",
    "                ],\n",
    "                value='acc',\n",
    "                clearable=False\n",
    "            )\n",
    "        ], width=4),\n",
    "    ]),\n",
    "\n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Graph(id='metric-graph'), width=12)\n",
    "    ]),\n",
    "\n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Graph(id='task-pie-chart'), width=6),\n",
    "        dbc.Col(dcc.Graph(id='framework-bar-chart'), width=6),\n",
    "    ]),\n",
    "\n",
    "], fluid=True)\n",
    "\n",
    "# Callbacks\n",
    "@app.callback(\n",
    "    Output('metric-graph', 'figure'),\n",
    "    Output('task-pie-chart', 'figure'),\n",
    "    Output('framework-bar-chart', 'figure'),\n",
    "    Input('metric-dropdown', 'value')\n",
    ")\n",
    "\n",
    "\n",
    "# Run app and display result inline in the notebook\n",
    "app.run_server(mode='inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_task_id_from_folder_name(folder_name):\n",
    "    name = re.findall(pattern=r\"_t_.*[0-9]\\.\", string=folder_name)\n",
    "    print(name)\n",
    "    if isinstance(name, list) and len(name) > 0:\n",
    "        return str(name[0])\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_t_5.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'_t_5.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_task_id_from_folder_name(folder_name=\"randomforest.openml_t_5.test.docker.20241016T141320\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = openml.tasks.get_task(10, download_data=False, download_qualities= False)\n",
    "t.dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automlb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
