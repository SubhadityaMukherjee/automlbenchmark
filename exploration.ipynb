{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dash import dcc, html\n",
    "from flask import Flask, render_template, jsonify, request, send_file\n",
    "from glob import glob\n",
    "from interpret import set_visualize_provider\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from interpret.provider import InlineProvider\n",
    "from itables import to_html_datatable\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from ollama import ChatResponse\n",
    "from ollama import chat\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional, Union\n",
    "import base64\n",
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "import io\n",
    "import json\n",
    "import markdown\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openml\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import re\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from typing import Any\n",
    "from src.utils import OpenMLTaskHandler\n",
    "\n",
    "matplotlib.use(\"agg\")\n",
    "set_visualize_provider(InlineProvider())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def safe_load_file(file_path, file_type) -> Union[pd.DataFrame, dict, None]:\n",
    "    \"\"\"\n",
    "    This function is responsible for safely loading a file. It returns None if the file is not found or if there is an error loading the file.\n",
    "    \"\"\"\n",
    "    if file_type == \"json\":\n",
    "        try:\n",
    "            with open(str(Path(file_path)), \"r\") as f:\n",
    "                return json.load(f)\n",
    "        except:\n",
    "            return None\n",
    "    elif file_type == \"pd\":\n",
    "        try:\n",
    "            return pd.read_csv(str(file_path))\n",
    "        except:\n",
    "            return None\n",
    "    elif file_type == \"textdict\":\n",
    "        try:\n",
    "            with open(file_path, \"r\") as f:\n",
    "                return json.loads(f.read())\n",
    "        except:\n",
    "            return None\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data report generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReportGenerator:\n",
    "    def __init__(self, generated_ebm_report_dir):\n",
    "        self.generated_ebm_report_dir = generated_ebm_report_dir\n",
    "\n",
    "    def run_ebm_on_dataset(self, dataset_id, X_train, y_train):\n",
    "        try:\n",
    "            ebm = ExplainableBoostingClassifier(random_state=42)\n",
    "            ebm.fit(X_train, y_train)\n",
    "            ebm_global = ebm.explain_global().data()\n",
    "            names, scores = ebm_global[\"names\"], ebm_global[\"scores\"]\n",
    "            return self.generate_ebm_report(names, scores)\n",
    "        except Exception as e:\n",
    "            return \"<div>Unable to generate feature importance report</div>\"\n",
    "\n",
    "    def generate_ebm_report(self, names, scores):\n",
    "        df = pd.DataFrame({\"Feature\": names, \"Score\": scores})\n",
    "        # sort the features by score\n",
    "        df = df.sort_values(by=\"Score\", ascending=False)\n",
    "        return to_html_datatable(df, index=False, table_id=\"feature-importance\")\n",
    "\n",
    "    def get_data_and_split(self, dataset_id):\n",
    "        dataset = openml.datasets.get_dataset(dataset_id=dataset_id)\n",
    "        X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
    "        X = pd.get_dummies(X, prefix_sep=\".\").astype(float)\n",
    "        y, y_categories = y.factorize()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "        return X, y, X_train, y_train\n",
    "\n",
    "    def get_feature_distribution(self, X):\n",
    "        try:\n",
    "            # return to_html_datatable(X.describe().T)\n",
    "            return f'<div id=\"feature-distribution\">{to_html_datatable(X.describe().T)}</div>'\n",
    "        except Exception as e:\n",
    "            return \"<div>Unable to generate feature distribution</div>\"\n",
    "\n",
    "    def get_missing_value_count(self, X):\n",
    "        try:\n",
    "            count = pd.DataFrame(X.isnull().sum(), columns=[\"Missing Value Count\"])\n",
    "            count.sort_values(by=\"Missing Value Count\", ascending=False, inplace=True)\n",
    "            return to_html_datatable(count, index=True)\n",
    "        except Exception as e:\n",
    "            return \"<div>Unable to generate missing value count</div>\"\n",
    "\n",
    "    def generate_data_summary_table(self, X):\n",
    "        cols = X.columns[:100]  # Limit to the first 100 columns\n",
    "        col_visualization = {}\n",
    "\n",
    "        for col in cols:\n",
    "            try:\n",
    "                X[col] = X[col].astype(str)\n",
    "                number_of_unique_values = len(X[col].unique())\n",
    "\n",
    "                # Handle small unique value counts with histograms\n",
    "                if number_of_unique_values < 10:\n",
    "                    fig = px.histogram(X, x=col)\n",
    "                    fig.update_layout(\n",
    "                        title=None,\n",
    "                        xaxis_title=col,\n",
    "                        yaxis_title=None,\n",
    "                        width=300,\n",
    "                        height=300,\n",
    "                    )\n",
    "                else:\n",
    "                    # Show the first 2 most frequent values and the rest as \"other\"\n",
    "                    value_counts = X[col].value_counts()\n",
    "                    value_counts = value_counts.sort_values(ascending=False)\n",
    "                    if len(value_counts) > 2:\n",
    "                        value_counts = pd.concat(\n",
    "                            [\n",
    "                                value_counts[:2],\n",
    "                                pd.Series([value_counts[2:].sum()], index=[\"Other\"]),\n",
    "                            ]\n",
    "                        )\n",
    "\n",
    "                    fig = px.pie(\n",
    "                        names=value_counts.index,\n",
    "                        values=value_counts.values,\n",
    "                        title=None,\n",
    "                        width=300,\n",
    "                        height=300,\n",
    "                    )\n",
    "                # Convert figure to HTML\n",
    "                encoded_image = fig.to_html(full_html=False, include_plotlyjs=\"cdn\")\n",
    "                col_visualization[col] = encoded_image\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log the error and use a placeholder for failed visualizations\n",
    "                col_visualization[col] = \"\"\n",
    "                print(f\"Error processing column '{col}': {str(e)}\")\n",
    "\n",
    "        try:\n",
    "            # Create a DataFrame row for visualizations\n",
    "            visualization_row = pd.DataFrame([col_visualization])\n",
    "\n",
    "            # Insert the visualization row at the second position\n",
    "            X_with_visualizations = pd.concat(\n",
    "                [X.head(0), visualization_row, X.head(10).iloc[0:]], ignore_index=True\n",
    "            )\n",
    "\n",
    "            if X_with_visualizations is not None:\n",
    "                # Generate the table rows and columns\n",
    "                table_headers = \"\".join(\n",
    "                    [\"<th>{}</th>\".format(col) for col in X_with_visualizations.columns]\n",
    "                )\n",
    "                table_rows = \"\".join(\n",
    "                    [\n",
    "                        \"<tr>\"\n",
    "                        + \"\".join([\"<td>{}</td>\".format(cell) for cell in row])\n",
    "                        + \"</tr>\"\n",
    "                        for row in X_with_visualizations.values\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # Add JavaScript for pagination\n",
    "                html = f\"\"\"\n",
    "                <body>\n",
    "                    <div class=\"container\" style=\"overflow-x:auto;\">\n",
    "                        <h2>Data Summary Table</h2>\n",
    "                        <table id=\"dataTable\" class=\"table table-striped table-bordered\" style=\"display:none;\">\n",
    "                            <thead>\n",
    "                                <tr>{table_headers}</tr>\n",
    "                            </thead>\n",
    "                            <tbody>\n",
    "                                {table_rows}\n",
    "                            </tbody>\n",
    "                        </table>\n",
    "                        <button id=\"prevBtn\">Previous</button>\n",
    "                        <button id=\"nextBtn\">Next</button>\n",
    "                    </div>\n",
    "                    \"\"\" + \"\"\"\n",
    "                    <script>\n",
    "                        const table = document.getElementById('dataTable');\n",
    "                        const cols = table.querySelectorAll('thead th');\n",
    "                        const rows = table.querySelectorAll('tbody tr');\n",
    "                        const totalCols = cols.length;\n",
    "                        const colsPerPage = 10;\n",
    "                        let startCol = 0;\n",
    "\n",
    "                        function updateTable() {\n",
    "                            // Hide all columns\n",
    "                            cols.forEach((col, index) => {\n",
    "                                col.style.display = 'none';\n",
    "                            });\n",
    "                            rows.forEach(row => {\n",
    "                                Array.from(row.children).forEach((cell, index) => {\n",
    "                                    cell.style.display = 'none';\n",
    "                                });\n",
    "                            });\n",
    "\n",
    "                            // Show only the columns for the current page\n",
    "                            for (let i = startCol; i < Math.min(startCol + colsPerPage, totalCols); i++) {\n",
    "                                cols[i].style.display = '';\n",
    "                                rows.forEach(row => {\n",
    "                                    row.children[i].style.display = '';\n",
    "                                });\n",
    "                            }\n",
    "\n",
    "                            // Show/hide pagination buttons\n",
    "                            document.getElementById('prevBtn').disabled = startCol === 0;\n",
    "                            document.getElementById('nextBtn').disabled = startCol + colsPerPage >= totalCols;\n",
    "                        }\n",
    "\n",
    "                        document.getElementById('prevBtn').addEventListener('click', () => {\n",
    "                            if (startCol > 0) {\n",
    "                                startCol -= colsPerPage;\n",
    "                                updateTable();\n",
    "                            }\n",
    "                        });\n",
    "\n",
    "                        document.getElementById('nextBtn').addEventListener('click', () => {\n",
    "                            if (startCol + colsPerPage < totalCols) {\n",
    "                                startCol += colsPerPage;\n",
    "                                updateTable();\n",
    "                            }\n",
    "                        });\n",
    "\n",
    "                        // Initialize the table\n",
    "                        table.style.display = '';\n",
    "                        updateTable();\n",
    "                    </script>\n",
    "                </body>\n",
    "                \"\"\"\n",
    "                return html\n",
    "\n",
    "        except Exception as e:\n",
    "            return \"<div>Unable to generate data summary table</div>\"\n",
    "\n",
    "\n",
    "    def generate_data_report_for_dataset(self, dataset_id):\n",
    "        report_path = f\"{self.generated_ebm_report_dir}/{dataset_id}_report.html\"\n",
    "        if os.path.exists(report_path):\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                X, y, X_train, y_train = self.get_data_and_split(dataset_id)\n",
    "\n",
    "                ebm_report = self.run_ebm_on_dataset(dataset_id, X_train, y_train)\n",
    "                missing_value_count = self.get_missing_value_count(X)\n",
    "\n",
    "                # combine the feature distribution and class imbalance reports if they are pd.DataFrame\n",
    "                # if isinstance(missing_value_count, pd.DataFrame) and isinstance(ebm_report, pd.DataFrame):\n",
    "                #     # ebm_report = pd.concat([missing_value_count, ebm_report], axis=1)\n",
    "                #     # merge\n",
    "                #     ebm_report = pd.merge(ebm_report, missing_value_count, left_index=True, right_index=True)\n",
    "                #     ebm_report = to_html_datatable(ebm_report, index=True,table_id=\"feature-importance-missing-value\")\n",
    "                # feature_distribution = self.get_feature_distribution(X)\n",
    "                # class_imbalance_report = self.class_imbalance(y)\n",
    "                data_summary_table = self.generate_data_summary_table(X)\n",
    "\n",
    "                report_html = f\"\"\"\n",
    "                        <h1>Extra Dataset Information</h1>\n",
    "                        <div>\n",
    "                            {data_summary_table}\n",
    "                        </div>\n",
    "                        <h2>Additional Details</h2>\n",
    "                        <h2>Feature Importance</h2>\n",
    "                        <div>\n",
    "                            {ebm_report}\n",
    "                        </div>\n",
    "                        <h2>Missing Value Count</h2>\n",
    "                        <div>\n",
    "                            {missing_value_count}\n",
    "                        </div>\n",
    "                    \"\"\"\n",
    "\n",
    "                report_path = (\n",
    "                    f\"{self.generated_ebm_report_dir}/{dataset_id}_report.html\"\n",
    "                )\n",
    "                with open(report_path, \"w\") as f:\n",
    "                    f.write(report_html)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing dataset {dataset_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResultCollector:\n",
    "    def __init__(self, path: str = \"../data/results/*\"):\n",
    "        self.experiment_directory = Path(path)\n",
    "        self.all_run_paths = glob(pathname=str(self.experiment_directory))\n",
    "        self.all_results = pd.DataFrame()\n",
    "        self.openml_task_handler = OpenMLTaskHandler()\n",
    "        # Required columns\n",
    "        self.required_columns = {\n",
    "            \"metric\",\n",
    "            \"result\",\n",
    "            \"framework\",\n",
    "            \"dataset_id\",\n",
    "            \"id\",\n",
    "            \"task\",\n",
    "            \"predict_duration\",\n",
    "            \"models\",\n",
    "        }\n",
    "\n",
    "        # Define how to find the best result for the metric\n",
    "        self.metric_used_dict = {\n",
    "            \"auc\": lambda x: x.max(),\n",
    "            \"neg_logloss\": lambda x: x.min(),\n",
    "        }\n",
    "\n",
    "    def get_dataset_description_from_id(self, dataset_id: int) -> Optional[str]:\n",
    "        dataset_id = int(dataset_id)\n",
    "        return openml.datasets.get_dataset(dataset_id).description\n",
    "\n",
    "    def collect_all_run_info_to_df(self):\n",
    "        \"\"\"\n",
    "        This function is responsible for loading all the results files from the runs and storing them in self.all_results. This is further used to generate the dashboard.\n",
    "        \"\"\"\n",
    "        all_results_list = []  # Temporary list to store individual DataFrames\n",
    "\n",
    "        for run_path in tqdm(self.all_run_paths, total=len(self.all_run_paths)):\n",
    "            run_path = Path(run_path)\n",
    "            results_file_path = run_path / \"results.csv\"\n",
    "\n",
    "            # Load results file if it exists\n",
    "            results_file = safe_load_file(results_file_path, \"pd\")\n",
    "\n",
    "            # If results file is loaded, proceed to process it\n",
    "            if results_file is not None:\n",
    "                # Get the model path specific to this run_path\n",
    "                models_path_list = list((run_path / \"models\").rglob(\"models.*\"))\n",
    "                leaderboard_path_list = list(\n",
    "                    (run_path / \"models\").rglob(\"leaderboard.*\")\n",
    "                )\n",
    "                # models_path = str(models_path_list[0]) if len(models_path_list) >0 else None\n",
    "\n",
    "                if len(models_path_list) > 0:\n",
    "                    models_path = str(models_path_list[0])\n",
    "                elif len(leaderboard_path_list) > 0:\n",
    "                    models_path = str(leaderboard_path_list[0])\n",
    "                else:\n",
    "                    models_path = None\n",
    "\n",
    "                # Add the model path as a new column in the current results_file DataFrame\n",
    "                results_file[\"models\"] = models_path\n",
    "\n",
    "                # Get the dataset ID for each row in the results file\n",
    "                try:\n",
    "                    results_file[\"dataset_id\"] = results_file[\"id\"].apply(\n",
    "                        self.openml_task_handler.get_dataset_id_from_task_id\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    results_file[\"dataset_id\"] = None\n",
    "\n",
    "                results_file[\"dataset_description\"] = results_file[\"dataset_id\"].apply(\n",
    "                    self.get_dataset_description_from_id\n",
    "                )\n",
    "                results_file[\"dataset_description\"] = None\n",
    "\n",
    "                # Append the processed DataFrame to our list\n",
    "                all_results_list.append(results_file)\n",
    "\n",
    "        # Concatenate all individual DataFrames into self.all_results\n",
    "        if all_results_list:\n",
    "            self.all_results = pd.concat(all_results_list, ignore_index=True)\n",
    "\n",
    "    def validate_dataframe_and_add_extra_info(self):\n",
    "        # Validate DataFrame\n",
    "        if self.all_results is None or self.all_results.empty:\n",
    "            return \"Error: Provided DataFrame is empty or None.\"\n",
    "\n",
    "        # Handle duplicate frameworks by keeping the one with the best result\n",
    "        self.all_results = self.all_results.drop_duplicates(\n",
    "            subset=[\"framework\"], keep=\"first\"\n",
    "        )\n",
    "\n",
    "        # Add missing columns with default values\n",
    "        for column in self.required_columns:\n",
    "            if column not in self.all_results.columns:\n",
    "                self.all_results[column] = \"N/A\"\n",
    "\n",
    "    def __call__(self):\n",
    "        self.collect_all_run_info_to_df()\n",
    "        return self.all_results\n",
    "        # self.validate_dataframe_and_add_extra_info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate complete report for ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GenerateCompleteReportForDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_id: int,\n",
    "        collector_results,\n",
    "        GENERATED_REPORTS_DIR: str = \"../data/generated_reports\",\n",
    "        GENERATED_DATA_REPORT_DIR: str = \"../data/generated_data_reports\",\n",
    "        template_dir = \"./website_assets/templates/\",\n",
    "    ):\n",
    "        self.dataset_id = dataset_id\n",
    "        self.collector_results = collector_results\n",
    "        self.current_results = self.get_results_for_dataset_id(self.dataset_id)\n",
    "        self.jinja_environment = Environment(\n",
    "            loader=FileSystemLoader(template_dir)\n",
    "        )\n",
    "        self.generated_final_reports_dir = GENERATED_REPORTS_DIR\n",
    "        self.generated_data_reports_dir = GENERATED_DATA_REPORT_DIR\n",
    "        self.template_to_use = {\n",
    "            \"dataset_info\": \"data_information.html\",\n",
    "            \"best_result\": \"best_result_table.html\",\n",
    "            \"framework_table\": \"framework_table.html\",\n",
    "            \"metric_vs_result\": \"metric_vs_result.html\",\n",
    "        }\n",
    "        binary_metrics = [\n",
    "            \"auc\",\n",
    "            \"logloss\",\n",
    "            \"acc\",\n",
    "            \"balacc\",\n",
    "        ]  # available metrics: auc (AUC), acc (Accuracy), balacc (Balanced Accuracy), pr_auc (Precision Recall AUC), logloss (Log Loss), f1, f2, f05 (F-beta scores with beta=1, 2, or 0.5), max_pce, mean_pce (Max/Mean Per-Class Error).\n",
    "        multiclass_metrics = [\n",
    "            \"logloss\",\n",
    "            \"acc\",\n",
    "            \"balacc\",\n",
    "        ]  # available metrics: same as for binary, except auc, replaced by auc_ovo (AUC One-vs-One), auc_ovr (AUC One-vs-Rest). AUC metrics and F-beta metrics are computed with weighted average.\n",
    "        regression_metrics = [\n",
    "            \"rmse\",\n",
    "            \"r2\",\n",
    "            \"mae\",\n",
    "        ]  # available metrics: mae (Mean Absolute Error), mse (Mean Squared Error), msle (Mean Squared Logarithmic Error), rmse (Root Mean Square Error), rmsle (Root Mean Square Logarithmic Error), r2 (R^2).\n",
    "        timeseries_metrics = [\n",
    "            \"mase\",\n",
    "            \"mape\",\n",
    "            \"smape\",\n",
    "            \"wape\",\n",
    "            \"rmse\",\n",
    "            \"mse\",\n",
    "            \"mql\",\n",
    "            \"wql\",\n",
    "            \"sql\",\n",
    "        ]  # available metrics: mase (Mean Absolute Scaled Error), mape (Mean Absolute Percentage Error),\n",
    "        self.all_metrics = (\n",
    "            binary_metrics\n",
    "            + multiclass_metrics\n",
    "            + regression_metrics\n",
    "            + timeseries_metrics\n",
    "        )\n",
    "\n",
    "        # run the function to get the best result\n",
    "        self.framework_names = [\"Auto-sklearn\", \"H20AutoML\", \"AutoGluon\", \"All results\"]\n",
    "        self.process_fns = [\n",
    "            self.process_auto_sklearn_data(self.current_results),\n",
    "            self.get_rows_for_framework_from_df(\n",
    "                df=self.current_results, framework_name=\"H20AutoML\", top_n=10\n",
    "            ),\n",
    "            self.get_rows_for_framework_from_df(\n",
    "                df=self.current_results, framework_name=\"AutoGluon\", top_n=10\n",
    "            ),\n",
    "            self.get_rows_for_framework_from_df(\n",
    "                df=self.current_results, framework_name=\"All results\"\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        self.best_framework = \"\"\n",
    "        self.best_metric = \"\"\n",
    "        self.type_of_task = \"\"\n",
    "        self.dataset_id = \"\"\n",
    "        self.task_id = \"\"\n",
    "        self.task_name = \"\"\n",
    "        self.best_result_for_metric = \"\"\n",
    "        self.description = \"\"\n",
    "        self.metric_and_result = \"\"\n",
    "\n",
    "        self.get_best_result()\n",
    "\n",
    "    def get_results_for_dataset_id(self, dataset_id: int) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        This function returns the results for a given dataset_id. If no results are found, it returns None.\n",
    "        \"\"\"\n",
    "        results_for_dataset = self.collector_results[\n",
    "            self.collector_results[\"dataset_id\"] == dataset_id\n",
    "        ]\n",
    "        if results_for_dataset.empty:\n",
    "            return None\n",
    "        return results_for_dataset\n",
    "\n",
    "    def get_best_result(self):\n",
    "        \"\"\"\n",
    "        This function returns the best result from the current_results DataFrame. It first sorts the DataFrame based on the metric used and then returns the best result.\n",
    "        \"\"\"\n",
    "        if self.current_results is None:\n",
    "            return None\n",
    "        metric_used = self.current_results[\"metric\"].iloc[0]\n",
    "        if metric_used in [\"auc\", \"acc\", \"balacc\"]:\n",
    "            # Since higher value is better we sort in descending order\n",
    "            sort_in_ascending_order = False\n",
    "        elif metric_used in [\"logloss\", \"neg_logloss\"]:\n",
    "            # Since lower value is better we sort in ascending order\n",
    "            sort_in_ascending_order = True\n",
    "        else:\n",
    "            sort_in_ascending_order = False\n",
    "\n",
    "        sorted_results = self.current_results.sort_values(\n",
    "            by=\"result\", ascending=sort_in_ascending_order\n",
    "        ).head()\n",
    "\n",
    "        best_result = sorted_results.iloc[0]\n",
    "        self.best_framework = best_result.get(\"framework\", \"\")\n",
    "        self.best_metric = best_result.get(\"metric\", \"\")\n",
    "        self.type_of_task = best_result.get(\"type\", \"\")\n",
    "        self.dataset_id = best_result.get(\"dataset_id\", \"\")\n",
    "        self.task_id = \"https://\" + best_result.get(\"id\", \"\")\n",
    "        self.task_name = best_result.get(\"task\", \"\")\n",
    "        self.best_result_for_metric = best_result.get(\"result\", \"\")\n",
    "        self.description = best_result.get(\"dataset_description\", \"\")\n",
    "\n",
    "        # all metric columns that are in the dataframe and in the list of all metrics\n",
    "        metric_columns = [\n",
    "            col for col in self.current_results.columns if col in self.all_metrics\n",
    "        ]\n",
    "        all_metrics_present = []\n",
    "        for metric in metric_columns:\n",
    "            try:\n",
    "                all_metrics_present.append(self.current_results[metric].values[0])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        self.metric_and_result = \" \".join(\n",
    "            [\n",
    "                f\"The {metric} is {result} \"\n",
    "                for metric, result in zip(metric_columns, all_metrics_present)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def generate_best_result_table(self):\n",
    "        \"\"\"\n",
    "        This function generates the best result table using the best result information.\n",
    "        \"\"\"\n",
    "        template = self.jinja_environment.get_template(\n",
    "            self.template_to_use[\"best_result\"]\n",
    "        )\n",
    "        return template.render(\n",
    "            best_framework=self.best_framework,\n",
    "            best_metric=self.best_metric,\n",
    "            type_of_task=self.type_of_task,\n",
    "            dataset_id=self.dataset_id,\n",
    "            task_id=self.task_id,\n",
    "            task_name=self.task_name,\n",
    "        )\n",
    "\n",
    "    def generate_dataset_info(self):\n",
    "        \"\"\"\n",
    "        This function generates the dataset information table using the dataset information.\n",
    "        \"\"\"\n",
    "        template = self.jinja_environment.get_template(\n",
    "            self.template_to_use[\"dataset_info\"]\n",
    "        )\n",
    "        return template.render(\n",
    "            dataset_id=self.dataset_id,\n",
    "            task_name=self.task_name,\n",
    "        )\n",
    "\n",
    "    def process_auto_sklearn_data(self, df, top_n=10):\n",
    "        auto_sklearn_data = pd.DataFrame()\n",
    "        try:\n",
    "            auto_sklearn_rows = df[df[\"framework\"] == \"autosklearn\"]\n",
    "            # for each row, read the json file from the models column and get the model id and cost\n",
    "            for _, row in auto_sklearn_rows.iterrows():\n",
    "                models_path = row[\"models\"]\n",
    "                try:\n",
    "                    with open(models_path, \"r\") as f:\n",
    "                        models_file = json.load(f)\n",
    "                        for model in models_file:\n",
    "                            model_type = (\n",
    "                                \"sklearn_classifier\"\n",
    "                                if \"sklearn_classifier\" in models_file[model]\n",
    "                                else \"sklearn_regressor\"\n",
    "                            )\n",
    "\n",
    "                            auto_sklearn_data = pd.concat(\n",
    "                                [auto_sklearn_data, pd.DataFrame([models_file[model]])],\n",
    "                                ignore_index=True,\n",
    "                            )\n",
    "                except:\n",
    "                    pass\n",
    "                auto_sklearn_data = auto_sklearn_data.sort_values(\n",
    "                    by=\"cost\", ascending=True\n",
    "                ).head(top_n)\n",
    "                return to_html_datatable(\n",
    "                    auto_sklearn_data, caption=\"Auto Sklearn Models\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return \"<div></div>\"\n",
    "\n",
    "        # return auto_sklearn_data.to_html()\n",
    "\n",
    "    def get_rows_for_framework_from_df(\n",
    "        self, df: pd.DataFrame, framework_name, top_n=40\n",
    "    ):\n",
    "        try:\n",
    "            if framework_name == \"All results\":\n",
    "                # drop the description column if it exists\n",
    "                try:\n",
    "                    df.drop(\"dataset_description\", axis=1)\n",
    "                except:\n",
    "                    pass\n",
    "                return to_html_datatable(df, caption=\"Results by AutoML Framework\", table_id=\"all-framework-results\")\n",
    "            framework_rows: pd.DataFrame = df[df[\"framework\"] == framework_name][\n",
    "                \"models\"\n",
    "            ].values[0]\n",
    "            framework_data = safe_load_file(framework_rows, \"pd\")\n",
    "            if top_n is not None:\n",
    "                framework_data = framework_data.head(40)\n",
    "\n",
    "            return to_html_datatable(framework_data, caption=f\"{framework_name} Models\")\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    def generate_framework_table(self):\n",
    "        \"\"\"\n",
    "        This function generates the framework table using the framework_name information.\n",
    "        \"\"\"\n",
    "        complete_html = \"\"\n",
    "        for framework_name, process_fn in zip(self.framework_names, self.process_fns):\n",
    "            try:\n",
    "                complete_html += process_fn\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return f\"\"\"<div class=\"container\" style=\"margin-top: 20px; text-align: left;\">\n",
    "                <h2>{framework_name}</h2>\n",
    "                    {complete_html}\n",
    "                </div>\n",
    "                \"\"\"\n",
    "\n",
    "    def generate_dashboard_section(self):\n",
    "        dashboard_html = f\"\"\"\n",
    "        <div style=\"text-align: left; margin: 20px 0;\">\n",
    "            <h1>Framework Performance Dashboard</h1>\n",
    "        </div>\n",
    "\n",
    "        <div style=\"display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 30px; margin-bottom: 40px;\">\n",
    "            {self.graph_and_heading(\n",
    "                self.current_results,\n",
    "                self.best_metric.upper() + \"-task\",\n",
    "                \"task\",\n",
    "                \"result\",\n",
    "                \"framework\",\n",
    "                f\"{self.best_metric.upper()} of each Framework\",\n",
    "                \"1\",\n",
    "                \"This is a plot of the main metric used in the experiment against the result of the experiment for each framework for each task. Use this plot to compare the performance of each framework for each task.\",\n",
    "                \"bar\"\n",
    "            )}\n",
    "            {self.graph_and_heading(\n",
    "                self.current_results,\n",
    "                \"predict-duration-task\",\n",
    "                \"framework\",\n",
    "                \"predict_duration\",\n",
    "                \"framework\",\n",
    "                \"Predict Duration of each Framework\",\n",
    "                \"2\",\n",
    "                \"This is a plot of the prediction duration for each framework for each task. Use this plot to find the framework with the fastest prediction time.\",\n",
    "                \"bar\"\n",
    "            )}\n",
    "            {self.graph_and_heading(\n",
    "                self.current_results,\n",
    "                \"framework-performance\",\n",
    "                \"framework\",\n",
    "                \"result\",\n",
    "                \"framework\",\n",
    "                \"Performance of each Framework\",\n",
    "                \"1\",\n",
    "                \"This is a plot of the performance of each framework for each task. Use this plot find the best framework for the tasks.\",\n",
    "                \"bar\"\n",
    "            )}\n",
    "            {self.graph_and_heading(\n",
    "                self.current_results,\n",
    "                \"predict-duration-performance\",\n",
    "                \"predict_duration\",\n",
    "                \"result\",\n",
    "                \"framework\",\n",
    "                \"Predict Duration vs Performance\",\n",
    "                \"2\",\n",
    "                \"This is a scatter plot of the prediction duration against the performance of each framework for each task. Use this plot to find the best framework for the tasks.\",\n",
    "                \"scatter\"\n",
    "            )}\n",
    "        </div>\n",
    "\n",
    "        \"\"\"\n",
    "        return dashboard_html\n",
    "\n",
    "    def graph_and_heading(\n",
    "        self,\n",
    "        df,\n",
    "        graph_id,\n",
    "        x,\n",
    "        y,\n",
    "        color,\n",
    "        title,\n",
    "        grid_column,\n",
    "        description,\n",
    "        plot_type=\"bar\",\n",
    "    ):\n",
    "        try:\n",
    "            colors = px.colors.qualitative.Safe\n",
    "            if len(x) == 0:\n",
    "                return \"<div></div>\"\n",
    "\n",
    "            # use plotly to create the plot\n",
    "            if plot_type == \"bar\":\n",
    "                fig = px.bar(\n",
    "                    df,\n",
    "                    x=x,\n",
    "                    y=y,\n",
    "                    color=color,\n",
    "                    title=title,\n",
    "                    color_discrete_sequence=colors,\n",
    "                )\n",
    "            elif plot_type == \"scatter\":\n",
    "                fig = px.scatter(\n",
    "                    df,\n",
    "                    x=x,\n",
    "                    y=y,\n",
    "                    color=color,\n",
    "                    title=title,\n",
    "                    color_discrete_sequence=colors,\n",
    "                )\n",
    "\n",
    "            fig.update_layout(\n",
    "                title=title,\n",
    "                xaxis_title=x,\n",
    "                yaxis_title=y,\n",
    "            )\n",
    "            encoded_image = fig.to_html(full_html=False, include_plotlyjs=\"cdn\")\n",
    "\n",
    "            return f\"<div style='grid-column: {grid_column};'>{encoded_image}</div>\"\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return f\"<div style='grid-column: {grid_column};'><p>Error generating graph: {str(e)}</p></div>\"\n",
    "\n",
    "    def get_explanation_from_llm(self):\n",
    "        prompt_format = f\"\"\"For a dataset called {self.task_name} , the best framework is {self.best_framework} with a {self.best_metric} of {self.best_result_for_metric}. This is a {self.type_of_task} task. The results are as follows {self.metric_and_result}. For each metric, tell me if this is a good score (and why), and if it is not, how can I improve it? Keep your answer to the point.\n",
    "        The dataset description is: {self.description}\n",
    "    \"\"\"\n",
    "        response: ChatResponse = chat(\n",
    "            model=\"llama3.2\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt_format,\n",
    "                },\n",
    "            ],\n",
    "            options={\n",
    "                \"temperature\": 0.3,\n",
    "            },\n",
    "        )\n",
    "        response = response[\"message\"][\"content\"]\n",
    "        markdown_response = markdown.markdown(response)\n",
    "        return f'<div id=\"explanation-and-whats-next style=\"text-align: left; margin-bottom: 20px;\">{markdown_response}</div>'\n",
    "\n",
    "    def get_data_report(self):\n",
    "        try:\n",
    "            with open(\n",
    "                f\"{self.generated_data_reports_dir}/{self.dataset_id}_report.html\", \"r\"\n",
    "            ) as f:\n",
    "                return f.read()\n",
    "        except FileNotFoundError:\n",
    "            return \"<div><p>Feature importance not available for this dataset</p></div>\"\n",
    "\n",
    "    def generate_download_current_page_button(self):\n",
    "        return f\"\"\"\n",
    "        <a href=\"report_{self.dataset_id}.html\" download=\"report_{self.dataset_id}.html\" class=\"btn btn-primary\">Download Report</a>\n",
    "        \"\"\"\n",
    "\n",
    "    def __call__(self):\n",
    "        dataset_info = self.generate_dataset_info()\n",
    "        best_result_table = self.generate_best_result_table()\n",
    "        framework_table = self.generate_framework_table()\n",
    "        dashboard_section = self.generate_dashboard_section()\n",
    "        explanation = self.get_explanation_from_llm()\n",
    "        feature_importance = self.get_data_report()\n",
    "        # download_button = self.generate_download_current_page_button()\n",
    "        combined_html = self.jinja_environment.get_template(\n",
    "            \"complete_page.html\"\n",
    "        ).render(\n",
    "            dataset_info=dataset_info,\n",
    "            best_result_table=best_result_table,\n",
    "            framework_table=framework_table,\n",
    "            dashboard_section=dashboard_section,\n",
    "            explanation=explanation,\n",
    "            feature_importance=feature_importance,\n",
    "        )\n",
    "        with open(\n",
    "            Path(self.generated_final_reports_dir) / f\"report_{self.dataset_id}.html\",\n",
    "            \"w\",\n",
    "        ) as f:\n",
    "            f.write(combined_html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_report_script_for_dataset(\n",
    "    GENERATED_DATA_REPORT_DIR, GENERATED_REPORTS_DIR, dataset_id, result_path, template_dir\n",
    "):\n",
    "    # collect all the results from the runs\n",
    "    collector = ResultCollector(result_path)\n",
    "    all_results = collector()\n",
    "    drg = DataReportGenerator(GENERATED_DATA_REPORT_DIR)\n",
    "    try:\n",
    "        # generate the data report for all datasets\n",
    "        drg.generate_data_report_for_dataset(dataset_id=dataset_id)\n",
    "        # write complete report to a file\n",
    "        GenerateCompleteReportForDataset(\n",
    "            dataset_id=dataset_id,\n",
    "            collector_results=all_results,\n",
    "            GENERATED_DATA_REPORT_DIR=GENERATED_DATA_REPORT_DIR,\n",
    "            GENERATED_REPORTS_DIR=GENERATED_REPORTS_DIR,\n",
    "            template_dir=template_dir,\n",
    "        )()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating report for dataset {dataset_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATED_DATA_REPORT_DIR = Path(\"./data/generated_data_reports\")\n",
    "os.makedirs(GENERATED_DATA_REPORT_DIR, exist_ok=True)\n",
    "\n",
    "GENERATED_REPORTS_DIR = Path(\"./data/generated_reports\")\n",
    "GENERATED_REPORTS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c03cd7f3d884a7b921db09e85026a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_path=\"./data/results/*\"\n",
    "collector = ResultCollector(result_path)\n",
    "all_results = collector()\n",
    "drg = DataReportGenerator(GENERATED_DATA_REPORT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8ab71ff2e14168a6c7364aafa497dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smukherjee/.pyenv/versions/automlb/lib/python3.9/site-packages/interpret/glassbox/_ebm/_ebm.py:1019: UserWarning:\n",
      "\n",
      "Detected multiclass problem. Forcing interactions to 0. Multiclass interactions only have local explanations. They are not currently displayed in the global explanation visualizations. Set interactions=0 to disable this warning. If you still want multiclass interactions, this API accepts a list, and the measure_interactions function can be used to detect them.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_report_script_for_dataset(\n",
    "    GENERATED_DATA_REPORT_DIR=GENERATED_DATA_REPORT_DIR,\n",
    "    GENERATED_REPORTS_DIR=GENERATED_REPORTS_DIR,\n",
    "    dataset_id=2,\n",
    "    result_path=\"./data/results/*\",\n",
    "    template_dir=\"./src/website_assets/templates/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automlb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
